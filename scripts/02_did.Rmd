# (PART\*) Impact analysis {.unnumbered}

# Difference-in-difference

In this script, the treatment effect are computed from the control and treated units defined from the matching process. The treatment effect are first computed at PA level, then aggregated at country, region and world level to obtain average treatement effects.

DESCRIPTION

## Initial settings

```{r setup, include=FALSE, eval = FALSE}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())

```

```{r, eval = FALSE}
#Install some libraries
install.packages(c("tictoc", "fixest", "cobalt", "future", "progressr", "did", "latex2exp", "janitor", "stringi"))

# Load Libraries
library(dplyr)
library(stringi)
library(tictoc) #For timing
library(xtable)
library(tidyr)
library(stringr)
library(ggplot2) # For plotting
library(RColorBrewer)
library(ggrepel)
library(aws.s3)
library(fixest) #For estimating the models
library(did)
library(latex2exp)
library(janitor)
```

```{r message=FALSE, warning=FALSE, eval = FALSE}
#Import functions
source("scripts/functions/02_fns_did.R")
```

```{r, eval = F}
#Saving directories
##Temporary
tmp_did = paste(tempdir(), "did", sep = "/")
##SSPCloud
# save_dir = paste("impact_analysis/matching", Sys.Date(), sep = "/")
load_dir = paste("impact_analysis/matching", "2023-09-21", sep = "/")
save_dir = paste("impact_analysis/did", "2023-09-21", sep = "/")

#Load data 
## AFD
# data_pa =
#   #fread("data_tidy/BDD_PA_AFD_ie.csv" , encoding = "UTF-8")
#   aws.s3::s3read_using(
#   FUN = data.table::fread,
#   encoding = "UTF-8",
#   object = "data_tidy/BDD_PA_AFD_ie.csv",
#   bucket = "projet-afd-eva-ap",
#   opts = list("region" = "")) %>%
#   #Sangha trinational (555547988) created in 2012 actually gathers three former PAs
#   #in CAF (31458), CMR (1245) and COG (72332) implemented in
#   #1990, 2001 and 1993 respectively.
#   # Evaluating the trinational PA is not relevant here : our method relies on pre-treatment obervsations (for matching and DiD) and the outcome is likely to be affected by the initial PAs. On the other hand, evaluating the three earlier PAs might be irrelevant for us : are they funded by the AFD ?? In a first approach, the trinational is removed.
#   filter(is.na(wdpaid) == TRUE | wdpaid != 555547988)
##FAPBM
data_fapbm =
  #fread("data_tidy/BDD_PA_AFD_ie.csv" , encoding = "UTF-8")
  aws.s3::s3read_using(
  FUN = data.table::fread,
  encoding = "UTF-8",
  object = "data_tidy/BDD_PA_FAPBM.csv",
  bucket = "projet-afd-eva-ap",
  opts = list("region" = ""))
##MDG
data_pa =
  #fread("data_tidy/BDD_PA_AFD_ie.csv" , encoding = "UTF-8")
  aws.s3::s3read_using(
  FUN = data.table::fread,
  encoding = "UTF-8",
  object = "data_tidy/BDD_PA_MDG.csv",
  bucket = "projet-afd-eva-ap",
  opts = list("region" = ""))

# list_iso = data_pa %>%
#   filter(region == "Africa") %>%
#   filter(!(iso3 %in% c("ZZ")))
# list_iso = unique(list_iso$iso3)

list_iso = "MDG"

## SIOP project data 

data_fund = 
  #fread("data_tidy/BDD_PA_AFD_fund.csv")
  aws.s3::s3read_using(
  FUN = data.table::fread,
  encoding = "UTF-8",
  # Mettre les options de FUN ici
  object = "data_tidy/BDD_PA_AFD_fund.csv",
  bucket = "projet-afd-eva-ap",
  opts = list("region" = ""))
data_fund_nodupl = data_fund %>%
  group_by(id_projet) %>%
  slice(1) %>%
  ungroup() %>%
  dplyr::select(-c("mt_part_cofinancier_prevu_euro", "cofinanciers_siop")) %>%
  mutate(kfw = grepl("kfw", cofinanciers, ignore.case = TRUE),
         ffem = grepl("ffem", cofinanciers, ignore.case = TRUE))

## Report from technical departments
data_pa_report = 
  #read_delim("data_raw/BDD_PA_AFD.csv", delim = ";")
  s3read_using(readr::read_delim,
                delim = ";",
               show_col_types = FALSE,
                object = "data_raw/BDD_PA_AFD.csv",
                bucket = "projet-afd-eva-ap",
                opts = list("region" = "")) %>%
  mutate(key = paste(id_projet, nom_ap, wdpaid, sep = "_")) %>%
  group_by(key) %>%
  slice(1)

#Specify the period of study to create the mapme.bidiversity portfolio
## Start year
yr_first = 2000
## End year
yr_last = 2021

alpha = 0.05

```

## Computing treatment effects at PA level

```{r, eval = F}
#For each country in the list, the different steps of the pre-processing are performed
count_i = 0 #Initialize counter
max_i = length(list_iso) #Max value of the counter
tic = tic() #Start timer
#Initialize a dataframe to store annual deforestation rate for each PA
df_fl_annual_wolf = data.frame()
#Initialize a dataframe to store ATT  of all PA analyzed
df_fc_att = data.frame() #Effect on forest cover for matched units
#df_fc_att_unm = data.frame() #Effect on forest cover for unmacthed units
df_fl_att = data.frame() #Effect on deforestation relative to 2000
#Initialize a dataframe to store forest loss for visual evidence before/after matching
df_plot_forest_loss = data.frame()

for (i in list_iso)
{
  #Update counter and show progress
  count_i = count_i+1
  print(paste0(i, " : country ", count_i, "/", max_i))
  
  #Load the matching frame
  print("--Loading the list of PAs in the country considered")
  output_pa_i = fn_did_list_pa(iso = i, load_dir = load_dir)
  if(output_pa_i$is_ok == FALSE) {next} else list_pa_i = output_pa_i$list_pa
  
  count_j = 0
  max_j = length(list_pa_i)
  
  for(j in list_pa_i)
  {
    
    count_j = count_j+1
    print(paste0("WDPA ID ", j, " : ", count_j, "/", max_j))
    
    #Load the datasets
    # print("--Loading the working datasets")
    # df_j = fn_did_load_df(iso = i,
    #                       wdpaid = j, 
    #                       load_dir = load_dir,
    #                       ext_input = ".csv")
    # 
    # df_wide_m_j = df_j$df_matched_wide
    # df_long_m_j = df_j$df_matched_long
    # df_wide_unm_j = df_j$df_unmatched_wide
    # df_long_unm_j = df_j$df_unmatched_long
    
    #Compute annual deforestation rates in treated and control matched areas
    print("--Compute average deforestation rates Ã  la Wolf et al. 2021")
    output_wolf_m_j = fn_fl_wolf(iso = i, 
                                wdpaid = j, 
                                alpha = alpha, 
                                load_dir = load_dir,
                                ext_input = ".csv")
    
    if(output_wolf_m_j$is_ok == FALSE) {next} else df_fl_wolf_m_j = output_wolf_m_j$df_fl_wolf_m_j
    df_fl_annual_wolf = rbind(df_fl_annual_wolf, df_fl_wolf_m_j)
    
    #Compute treatment effects
    print("--Compute treatment effects")
    print("----For matched units")
    
    #For AFD projects (funding info)
    # output_att_m_j = fn_did_att_afd(iso = i, wdpaid = j, 
    #                       is_m = TRUE,
    #                   data_pa = data_pa,
    #                   data_fund = data_fund_nodupl,
    #                   data_report = data_pa_report,
    #                   alpha = alpha,
    #                   load_dir = load_dir,
    #                   ext_input = ".csv",
    #                   save_dir = save_dir)
    
    #For PAs in general (no funding info)
      output_att_m_j = fn_did_att_general(iso = i, wdpaid = j, 
                        is_m = TRUE,
                    data_pa = data_pa,
                    alpha = alpha,
                    load_dir = load_dir,
                    ext_input = ".csv",
                    save_dir = save_dir)
    
    if(output_att_m_j$is_ok == FALSE) {next} else df_att_m_j = output_att_m_j
    
    df_fc_att = rbind(df_fc_att, df_att_m_j$df_fc_att)
    df_fl_att = rbind(df_fl_att, df_att_m_j$df_fl_att)
        
    print("----For unmatched units")
    
    #For AFD PAs (funding info known)
    # df_att_unm_j = fn_did_att_afd(iso = i, wdpaid = j, 
    #                       is_m = FALSE,
    #                   data_pa = data_pa,
    #                   alpha = 0.05,
    #                   load_dir = load_dir,
    #                   ext_input = ".csv",
    #                   save_dir = save_dir)
    # df_fc_att_unm = rbind(df_fc_att_unm, df_att_unm_j$df_fc_att)
    # df_fl_att_unm = rbind(df_fl_att, df_att_unm_j$df_fl_att)
    
    #For genral PAs (funding info unknown)
    # df_att_unm_j = fn_did_att_general(iso = i, wdpaid = j, 
    #                       is_m = FALSE,
    #                   data_pa = data_pa,
    #                   alpha = 0.05,
    #                   load_dir = load_dir,
    #                   ext_input = ".csv",
    #                   save_dir = save_dir)    
    # df_fc_att_unm = rbind(df_fc_att_unm, df_att_unm_j$df_fc_att)
    # df_fl_att_unm = rbind(df_fl_att, df_att_unm_j$df_fl_att)
    
    #Plot visual evidence before-after matching
    print("--Plot visual evidence before-after matching")
    df_plot_forest_loss_j = fn_plot_forest_loss(iso = i, 
                                                wdpaid = j, 
                                                alpha = alpha,
                                                data_pa = data_pa, 
                                                load_dir = load_dir, 
                                                ext_input = ".csv", 
                                                save_dir = save_dir)
    df_plot_forest_loss = rbind(df_plot_forest_loss, df_plot_forest_loss_j)

  }  

  
  
}

#Finally save the ATT for every PA analyzed
aws.s3::s3write_using(
FUN = data.table::fwrite,
df_fc_att,
# Mettre les options de FUN ici
object = paste(save_dir, "data_fc_att.csv", sep = "/"),
bucket = "projet-afd-eva-ap",
opts = list("region" = ""))

aws.s3::s3write_using(
FUN = data.table::fwrite,
df_fl_att,
# Mettre les options de FUN ici bucket = , iso, wdpaid, sep = "/")
object = paste(save_dir, "data_fl_att.csv", sep = "/"),
bucket = "projet-afd-eva-ap",
opts = list("region" = ""))

toc = toc()


```

Compute aggregated treatment effects at country and region level

```{r, eval = FALSE}

#Finally ATT are aggregated by region and country
#For total avoided deforestation in ha, ATT are summed and so are CI.
## If the CI of an ATT is NA, then the CI for total ATT is NA also. Otherwise CI will be ## downward biased
#For avoided deforestation in % of 2000 forest cover, ATT are averaged and so are CI
## CI being NA is less a problem here as e use a mean, not a sum
# avg_att_fc_ctry = df_fc_att %>%
#   # group_by(wdpaid) %>%
#   # mutate(has_effect = att_pa[time == max(time)] > 0 & sig_95[time == max(time)] == TRUE) %>%
#   # ungroup() %>%
#   group_by(region, iso3, time) %>%
#   summarize(n_obs = n(),
#             att_per_mean = mean(att_per, na.rm = TRUE),
#             cband_lower_per = mean(cband_lower_per, na.rm = TRUE),
#             cband_upper_per = mean(cband_upper_per, na.rm = TRUE),
#             att_pa_tot = sum(att_pa, na.rm = TRUE),
#             cband_lower_pa = sum(cband_lower_pa, na.rm = FALSE),
#             cband_upper_pa = sum(cband_upper_pa, na.rm = FALSE)) 
#   
# avg_att_fc_region = df_fc_att %>%
#   group_by(region, time) %>%
#   summarize(n_obs = n(),
#             att_per_mean = mean(att_per, na.rm = TRUE),
#             cband_lower_per = mean(cband_lower_per, na.rm = TRUE),
#             cband_upper_per = mean(cband_upper_per, na.rm = TRUE),
#             att_pa_tot = sum(att_pa, na.rm = TRUE),
#             cband_lower_pa_95 = sum(cband_lower_pa, na.rm = FALSE),
#             cband_upper_pa = sum(cband_upper_pa, na.rm = FALSE))
# 
# avg_att_fl_ctry = df_fl_att %>%
#   group_by(region, iso3, time) %>%
#   summarize(n_obs = n(),
#             att_mean = mean(att, na.rm = TRUE),
#             cband_lower = mean(cband_lower, na.rm = TRUE),
#             cband_upper = mean(cband_upper, na.rm = TRUE))
# 
# avg_att_fl_region = df_fl_att %>%
#   group_by(region, time) %>%
#   summarize(n_obs = n(),
#             att_mean = mean(att, na.rm = TRUE),
#             cband_lower_95 = mean(cband_lower_95, na.rm = TRUE),
#             cband_upper_95 = mean(cband_upper_95, na.rm = TRUE))

avg_fl_annual_wolf_africa = df_fl_annual_wolf %>%
  filter(group == "Treated") %>%
  summarize(avg_FL_annual_wolf_pre = mean(avgFL_annual_wolf_pre, na.rm = TRUE),
            avg_FL_annual_wolf_pre_ci_up = mean(avgFL_annual_wolf_pre, na.rm = TRUE) +  qt((1-alpha)/2,df=21-1)*sd(avgFL_annual_wolf_pre, na.rm = TRUE)/sqrt(21),
            avg_FL_annual_wolf_pre_ci_lower = mean(avgFL_annual_wolf_pre, na.rm = TRUE) -  qt((1-alpha)/2,df=21-1)*sd(avgFL_annual_wolf_pre, na.rm = TRUE)/sqrt(21),
            med_FL_annual_wolf_pre_ci_up = median(avgFL_annual_wolf_pre, na.rm = TRUE),
            avg_FL_annual_wolf_tot = mean(avgFL_annual_wolf_tot, na.rm = TRUE),
            avg_FL_annual_wolf_tot_ci_up = mean(avgFL_annual_wolf_tot, na.rm = TRUE) +  qt((1-alpha)/2,df=21-1)*sd(avgFL_annual_wolf_tot, na.rm = TRUE)/sqrt(21),
            avg_FL_annual_wolf_tot_ci_lower = mean(avgFL_annual_wolf_tot, na.rm = TRUE) -  qt((1-alpha)/2,df=21-1)*sd(avgFL_annual_wolf_tot, na.rm = TRUE)/sqrt(21),
            med_FL_annual_wolf_tot_ci_up = median(avgFL_annual_wolf_tot, na.rm = TRUE)) 

avg_fl_annual_wolf_africa = df_fl_annual_wolf %>%
  filter(group == "Control") %>%
  summarize(avg_FL_annual_wolf_pre = mean(avgFL_annual_wolf_pre, na.rm = TRUE),
            avg_FL_annual_wolf_pre_ci_up = mean(avgFL_annual_wolf_pre, na.rm = TRUE) +  qt((1-alpha)/2,df=21-1)*sd(avgFL_annual_wolf_pre, na.rm = TRUE)/sqrt(21),
            avg_FL_annual_wolf_pre_ci_lower = mean(avgFL_annual_wolf_pre, na.rm = TRUE) -  qt((1-alpha)/2,df=21-1)*sd(avgFL_annual_wolf_pre, na.rm = TRUE)/sqrt(21),
            med_FL_annual_wolf_pre_ci_up = median(avgFL_annual_wolf_pre, na.rm = TRUE),
            avg_FL_annual_wolf_tot = mean(avgFL_annual_wolf_tot, na.rm = TRUE),
            avg_FL_annual_wolf_tot_ci_up = mean(avgFL_annual_wolf_tot, na.rm = TRUE) +  qt((1-alpha)/2,df=21-1)*sd(avgFL_annual_wolf_tot, na.rm = TRUE)/sqrt(21),
            avg_FL_annual_wolf_tot_ci_lower = mean(avgFL_annual_wolf_tot, na.rm = TRUE) -  qt((1-alpha)/2,df=21-1)*sd(avgFL_annual_wolf_tot, na.rm = TRUE)/sqrt(21),
            med_FL_annual_wolf_tot_ci_up = median(avgFL_annual_wolf_tot, na.rm = TRUE))

avg_fl_annual_wolf_country = df_fl_annual_wolf %>%
  filter(group == "Treated") %>%
  group_by(iso3) %>%
  summarize(avg_FL_annual_wolf_pre = mean(avgFL_annual_wolf_pre, na.rm = TRUE),
            avg_FL_annual_wolf_pre_ci_up = mean(avgFL_annual_wolf_pre, na.rm = TRUE) +  qt((1-alpha)/2,df=21-1)*sd(avgFL_annual_wolf_pre, na.rm = TRUE)/sqrt(21),
            avg_FL_annual_wolf_pre_ci_lower = mean(avgFL_annual_wolf_pre, na.rm = TRUE) -  qt((1-alpha)/2,df=21-1)*sd(avgFL_annual_wolf_pre, na.rm = TRUE)/sqrt(21),
            med_FL_annual_wolf_pre_ci_up = median(avgFL_annual_wolf_pre, na.rm = TRUE),
            avg_FL_annual_wolf_tot = mean(avgFL_annual_wolf_tot, na.rm = TRUE),
            avg_FL_annual_wolf_tot_ci_up = mean(avgFL_annual_wolf_tot, na.rm = TRUE) +  qt((1-alpha)/2,df=21-1)*sd(avgFL_annual_wolf_tot, na.rm = TRUE)/sqrt(21),
            avg_FL_annual_wolf_tot_ci_lower = mean(avgFL_annual_wolf_tot, na.rm = TRUE) -  qt((1-alpha)/2,df=21-1)*sd(avgFL_annual_wolf_tot, na.rm = TRUE)/sqrt(21),
            med_FL_annual_wolf_tot_ci_up = median(avgFL_annual_wolf_tot, na.rm = TRUE)) %>%
  ungroup() 
  

df_plot_forest_loss_agg = df_plot_forest_loss %>%
  group_by(matched, group, year) %>%
  summarize(tot_fc_rel00_ha = sum(fc_tot_rel00_ha, na.rm = TRUE),
            tot_fc_rel00_ha_ci_lower = sum(fc_tot_rel00_ha_ci_lower, na.rm = TRUE),
            tot_fc_rel00_ha_ci_upper = sum(fc_tot_rel00_ha_ci_upper, na.rm = TRUE),
              avg_fc_rel00_ha = mean(fc_tot_rel00_ha, na.rm = TRUE),
              avg_fc_rel00_ha_ci_lower = mean(fc_tot_rel00_ha_ci_lower, na.rm = TRUE),
              avg_fc_rel00_ha_ci_upper = mean(fc_tot_rel00_ha_ci_upper, na.rm = TRUE)) %>%
  ungroup() 

df_plot_forest_loss_agg_fapbm = df_plot_forest_loss %>%
  filter(wdpaid %in% unique(data_fapbm$wdpaid)) %>%
  group_by(matched, group, year) %>%
  summarize(tot_fc_rel00_ha = sum(fc_tot_rel00_ha, na.rm = TRUE),
            tot_fc_rel00_ha_ci_lower = sum(fc_tot_rel00_ha_ci_lower, na.rm = TRUE),
            tot_fc_rel00_ha_ci_upper = sum(fc_tot_rel00_ha_ci_upper, na.rm = TRUE),
              avg_fc_rel00_ha = mean(fc_tot_rel00_ha, na.rm = TRUE),
              avg_fc_rel00_ha_ci_lower = mean(fc_tot_rel00_ha_ci_lower, na.rm = TRUE),
              avg_fc_rel00_ha_ci_upper = mean(fc_tot_rel00_ha_ci_upper, na.rm = TRUE)) %>%
  ungroup()


year.max = 2021
fct.labs <- c("Before Matching", "After Matching")
names(fct.labs) <- c(FALSE, TRUE)

n_pa  = length(unique(df_plot_forest_loss$wdpaid))
  fct.labs <- c("Before Matching", "After Matching")
  names(fct.labs) <- c(FALSE, TRUE)
n_fapbm  = length(unique(data_pa_fapbm[data_pa_fapbm$wdpaid %in% unique(df_plot_forest_loss$wdpaid),]$wdpaid))


area_tot_ha = sum(data_pa[data_pa$wdpaid %in% unique(df_plot_forest_loss$wdpaid),]$area_km2, na.rm = TRUE) *100
area_fapbm_ha = sum(data_pa_fapbm[data_pa_fapbm$wdpaid %in% unique(df_plot_forest_loss$wdpaid),]$area_km2, na.rm = TRUE) *100

fig_forest_loss_agg_tot_fapbm = ggplot(data = filter(df_plot_forest_loss_agg_fapbm, year == year.max),
               aes(y = abs(tot_fc_rel00_ha), fill = as.factor(group), x = group)) %>%
    + geom_bar(position =  position_dodge(width = 0.8), stat = "identity", show.legend = FALSE) %>% 
    + geom_errorbar(aes(ymax=abs(tot_fc_rel00_ha_ci_upper), ymin=abs(tot_fc_rel00_ha_ci_lower)), width=0.3, colour="grey70", alpha=0.9, size=1) %>%
    + geom_label(aes(label = format(round(abs(tot_fc_rel00_ha), 0), big.mark = ","), y = 0),
                 vjust = -0.5,
                 color = "black",
                 show.legend = FALSE) %>%
    + scale_fill_brewer(name = "Group", palette = "Blues") %>%
    + labs(x = "",
           y = "Forest cover loss (ha)",
           title = paste("Total area deforested between 2000 and", year.max),
           subtitle = paste0("Sample : FAPBM protected areas in the analysis (" , n_fapbm, " areas covering ", format(area_fapbm_ha, big.mark = ","), "ha)"),
           caption = paste(((1-alpha)*100), "% confidence intervals")) %>%
    + facet_wrap(~matched,
                 labeller = labeller(matched = fct.labs))  %>%
    + theme_minimal() %>%
    + theme(
      axis.text.x = element_text(angle = 0, hjust = 0.5, vjust = 0.5),
      axis.text=element_text(size=11, color = "black"),
      axis.title=element_text(size=14, color = "black", face = "plain"),
      
      plot.caption = element_text(hjust = 0),
      plot.title = element_text(size=16, color = "black", face = "plain", hjust = 0),
      plot.subtitle = element_text(size=12, color = "black", face = "plain", hjust = 0),
      
      strip.text = element_text(color = "black", size = 12),
      
      #legend.position = "bottom",
      #legend.title = element_blank(),
      legend.text=element_text(size=10),
      #legend.spacing.x = unit(1.0, 'cm'),
      legend.spacing.y = unit(0.75, 'cm'),
      legend.key.size = unit(2, 'line'),
      
      panel.grid.major.x = element_line(color = 'grey80', linewidth = 0.3, linetype = 1),
      panel.grid.minor.x = element_line(color = 'grey80', linewidth = 0.2, linetype = 2),
      panel.grid.major.y = element_line(color = 'grey80', linewidth = 0.3, linetype = 1),
      panel.grid.minor.y = element_line(color = 'grey80', linewidth = 0.2, linetype = 2)
    )
fig_forest_loss_agg_tot_fapbm


fig_forest_loss_agg_avg_fapbm = ggplot(data = filter(df_plot_forest_loss_agg_fapbm, year == year.max),
               aes(y = abs(avg_fc_rel00_ha), fill = as.factor(group), x = group)) %>%
    + geom_bar(position =  position_dodge(width = 0.8), stat = "identity", show.legend = FALSE) %>% 
    + geom_errorbar(aes(ymax=abs(avg_fc_rel00_ha_ci_upper), ymin=abs(avg_fc_rel00_ha_ci_lower)), width=0.3, colour="grey70", alpha=0.9, size=1) %>%
    + geom_label(aes(label = format(round(abs(avg_fc_rel00_ha), 0), big.mark = ","), y = 0),
                 vjust = -0.5,
                 color = "black",
                 show.legend = FALSE) %>%
    + scale_fill_brewer(name = "Group", palette = "Blues") %>%
    + labs(x = "",
           y = "Forest cover loss (ha)",
           title = paste("Area deforested in protected areas on average, between 2000 and", year.max),
           subtitle = paste0("Sample : FAPBM protected areas in the analysis (" , n_fapbm, " areas covering ", format(area_fapbm_ha, big.mark = ","), " ha)"),
           caption = paste(((1-alpha)*100), "% confidence intervals")) %>%
    + facet_wrap(~matched,
                 labeller = labeller(matched = fct.labs))  %>%
    + theme_minimal() %>%
    + theme(
      axis.text.x = element_text(angle = 0, hjust = 0.5, vjust = 0.5),
      axis.text=element_text(size=11, color = "black"),
      axis.title=element_text(size=14, color = "black", face = "plain"),
      
      plot.caption = element_text(hjust = 0),
      plot.title = element_text(size=16, color = "black", face = "plain", hjust = 0),
      plot.subtitle = element_text(size=12, color = "black", face = "plain", hjust = 0),
      
      strip.text = element_text(color = "black", size = 12),
      
      #legend.position = "bottom",
      #legend.title = element_blank(),
      legend.text=element_text(size=10),
      #legend.spacing.x = unit(1.0, 'cm'),
      legend.spacing.y = unit(0.75, 'cm'),
      legend.key.size = unit(2, 'line'),
      
      panel.grid.major.x = element_line(color = 'grey80', linewidth = 0.3, linetype = 1),
      panel.grid.minor.x = element_line(color = 'grey80', linewidth = 0.2, linetype = 2),
      panel.grid.major.y = element_line(color = 'grey80', linewidth = 0.3, linetype = 1),
      panel.grid.minor.y = element_line(color = 'grey80', linewidth = 0.2, linetype = 2)
    )
fig_forest_loss_agg_avg_fapbm


```

```{r}
##Saving plots
tmp = paste(tempdir(), "fig", sep = "/")

ggsave(paste(tmp, "fig_forest_loss_agg_tot_fapbm.png", sep = "/"),
       plot = fig_forest_loss_agg_tot_fapbm,
       device = "png",
       height = 6, width = 9)

ggsave(paste(tmp, "fig_forest_loss_agg_avg_fapbm.png", sep = "/"),
       plot = fig_forest_loss_agg_avg_fapbm,
       device = "png",
       height = 6, width = 9)

  files <- list.files(tmp, full.names = TRUE)
##Add each file in the bucket (same foler for every file in the temp)
for(f in files) 
{
  cat("Uploading file", paste0("'", f, "'"), "\n")
  aws.s3::put_object(file = f, 
                     bucket = paste("projet-afd-eva-ap", save_dir, sep = "/"),
                     region = "", show_progress = TRUE)
}
do.call(file.remove, list(list.files(tmp, full.names = TRUE)))
```

## Results in figures and tables

```{r}
# Remove the PAs we do not xant to take (bad matching, bad pre-treatment parallel trend. This is done visually here, but we should have quantitative criteria in the future)
list_wdpa_bad = c(352240, #matching
                  352240, #matching
                  555542728, #pre-treatment parallel trend
                  555548846 #no CI
                  )
list_wdpa_focus = unique(data_fapbm$wdpaid)

df_fc_att_tidy = df_fc_att %>%
  filter(!wdpaid %in% list_wdpa_bad)
df_fl_att_tidy = df_fl_att %>%
  filter(!wdpaid %in% list_wdpa_bad)

#For AFD PAs (funding info)
# fn_plot_att_afd(df_fc_att = df_fc_att_tidy,
#             df_fl_att = df_fl_att_tidy, 
#             alpha = alpha,
#             save_dir = save_dir)

#For non-AFD PAs (no funding info)
fn_plot_att_general(df_fc_att = df_fc_att_tidy,
            df_fl_att = df_fl_att_tidy, 
            list_focus = list_wdpa_focus,
            alpha = alpha,
            save_dir = save_dir)

```

## Old code

```{r}
data_pa_nomarine = data_pa %>%
  filter(marine %in% c(0,1))
#Dataframe of PAs restricted to Africa
data_pa_africa = data_pa %>%
  filter(region == "Africa")
data_pa_africa_nomarine = data_pa %>%
  filter(region == "Africa" & marine %in% c(0,1))
#Number of PAs (total, WDPA, not WDPA)
n_pa_africa = nrow(data_pa_africa)
n_pa_africa_wdpa = nrow(filter(data_pa_africa, is.na(wdpaid) == FALSE))
n_pa_africa_nowdpa = nrow(filter(data_pa_africa, is.na(wdpaid) == TRUE))
#Dataframes of PA analyzed according to our criteria
df_pa_africa_analyzed = data_pa_africa %>%
  filter(is.na(wdpaid) == FALSE & area_km2 > 1 & marine %in% c(0,1) & status_yr >= 2002 & status != "Proposed")
#Number of PA analyzed, corresponding number of countries
n_pa_africa_analyzed = nrow(df_pa_africa_analyzed)
n_ctry_africa_analyzed = length(unique(df_pa_africa_analyzed$iso3))

#Statistics on resolutions
df_res = df_fc_att %>%
  group_by(wdpaid) %>%
  slice(1) %>%
  ungroup() %>%
  summarize(min_res_ha = min(res_ha),
            max_res_ha = max(res_ha),
            mean_res_ha = mean(res_ha),
            med_res_ha = median(res_ha)) %>%
  round(2)

#Statisitcs on funding
data_stat_fund = 
  #fread("data_tidy/BDD_PA_AFD_fund.csv")
  aws.s3::s3read_using(
  FUN = data.table::fread,
  encoding = "UTF-8",
  # Mettre les options de FUN ici
  object = "data_tidy/BDD_PA_AFD_fund.csv",
  bucket = "projet-afd-eva-ap",
  opts = list("region" = ""))

wdpaid_marine = data_pa %>%
  filter(is.na(wdpaid) == FALSE) %>%
  select(c("wdpaid", "marine"))

data_fund_africa = data_stat_fund %>%
  dplyr::distinct(id_projet, .keep_all = TRUE) %>%
  filter(region == "Africa")  
tot_funding_africa = sum(data_fund_africa$mt_fin_global_af_d_prevu_devise, na.rm = TRUE)

data_fund_africa_nomarine = data_stat_fund %>%
  left_join(wdpaid_marine, by = "wdpaid") %>%
  filter(marine %in% c(0,1)) %>%
  dplyr::distinct(id_projet, .keep_all = TRUE) %>%
  filter(region == "Africa") 
tot_funding_africa_nomarine = sum(data_fund_africa_nomarine$mt_fin_global_af_d_prevu_devise, na.rm = TRUE)

data_fund_nomarine = data_stat_fund %>%
  left_join(wdpaid_marine, by = "wdpaid") %>%
  filter(marine %in% c(0,1)) %>%
  dplyr::distinct(id_projet, .keep_all = TRUE) 
tot_funding_nomarine = sum(data_fund_nomarine$mt_fin_global_af_d_prevu_devise, na.rm = TRUE)
```
