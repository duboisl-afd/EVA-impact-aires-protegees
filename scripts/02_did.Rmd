# Difference-in-difference

In this script are performed the last steps of the analysis. Using the datasets of matched treated and control units, it is possible to compute the effect of the conservation for each protected area. Then these results can be aggregated at country and region level. Some secondary metrics are also computed : the annual deforestation rates faced by treated and control units, before and after treatment (à la Wolf et al. 2021); the total and average forest loss across groups of protected areas on the period considered, compared to the value in control units, before and after matching.

The process consists of the following steps.

1.  For a given country, load the list of protected areas whose observational units (treated pixels composing this area) have been matched to control units.

2.  For each protected area in this list :

    1.  Compute annual deforestation rates : before and after treatment for treated units, on the whole period for control units.

    2.  Compute treatment effect from matched treated and control pixels. Note that two functions exist depending on the portfolio analyzed : for AFD supported protected areas, funding-related information are added to the analysis (e.g year of funding). For the others, a general script is used.

    3.  Compute treatment effect for unmatched treated and control pixels. This can be useful to assess the bias induced by the selection into treatment. In other words, the bias due to not using matching. Indeed protected areas tend to be located in region less prone to deforestation (e.g further away from cities or roads). Simply using non-protected areas as a counterfactual, without matching, would then overestimate the effect of the conservation program. Note that this step can be time and computationally intensive, because the number of unmatched units is higher than the number of matched ones. It is usually skipped.

    4.  Plot the total area deforested on the period considered, in the protected area and its counterfactual, before and after matching. This is useful to illustrate the bias described above. Note that ideally the deforestation estimated in the protected area before and after matching should be the same. If note, the matched treated units are not representative of the overall protected area and the a local treatment effect will be estimated.

3.  The treatment effects, annual deforestation rates and estimations of total deforested area computed for each protected areas are gathered in specific datasets. This makes it possible to compute metrics aggregated at country and region level. These datastes are saved to the storage.

4.  If relevant, results of the analysis at protected area level are aggregated at country and region level.

5.  Finally, figures and tables are created to display the results.

## Initial settings

Configuring the Rmarkdown.

```{r setup, include=FALSE, eval = FALSE}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) 
```

Downloading and importing relevant packages.

```{r, eval = FALSE}
#Install some libraries
install.packages(c("tictoc", "fixest", "cobalt", "future", "progressr", "did", "latex2exp", "janitor", "stringi"))

# Load Libraries
library(dplyr)
library(stringi)
library(tictoc) #For timing
library(xtable)
library(tidyr)
library(stringr)
library(ggplot2) # For plotting
library(RColorBrewer)
library(ggrepel)
library(aws.s3)
library(fixest) #For estimating the models
library(did)
library(latex2exp)
library(janitor)
```

To keep this document concise, each step calls a function defined in a R script. Interested reader can delve deeper into the data processing by looking at this script. The following chunk load the functions in the workspace.

```{r message=FALSE, warning=FALSE, eval = FALSE}
#Import functions
source("scripts/functions/02_fns_did.R")
```

## Load datasets and define critical parameters

```{r, eval = F}
#Define working directories
##Temporary directory
tmp_did = paste(tempdir(), "did", sep = "/")
##Loading and saving directories on the storage. This is either define on today's date, or by a user-defined date.
#load_dir = paste("impact_analysis/matching", Sys.Date(), sep = "/")
load_dir = paste("impact_analysis/matching", "2023-10-23", sep = "/")
#save_dir = paste("impact_analysis/did", Sys.Date(), sep = "/")
save_dir = paste("impact_analysis/did", "2023-10-23", sep = "/")

## Dataset specific to the PAs portfolio to analyze. Only one is selected depending on the analysis one wants to perform. 

## AFD portfolio
# data_pa =
#   #fread("data_tidy/BDD_PA_AFD_ie.csv" , encoding = "UTF-8")
#   aws.s3::s3read_using(
#   FUN = data.table::fread,
#   encoding = "UTF-8",
#   object = "data_tidy/BDD_PA_AFD_ie.csv",
#   bucket = "projet-afd-eva-ap",
#   opts = list("region" = "")) %>%
#   #Sangha trinational (555547988) created in 2012 actually gathers three former PAs
#   #in CAF (31458), CMR (1245) and COG (72332) implemented in
#   #1990, 2001 and 1993 respectively.
#   # Evaluating the trinational PA is not relevant here : our method relies on pre-treatment obervsations (for matching and DiD) and the outcome is likely to be affected by the initial PAs. On the other hand, evaluating the three earlier PAs might be irrelevant for us : are they funded by the AFD ?? In a first approach, the trinational is removed.
#   filter(is.na(wdpaid) == TRUE | wdpaid != 555547988)

##FAPBM
# data_fapbm =
#   #fread("data_tidy/BDD_PA_AFD_ie.csv" , encoding = "UTF-8")
#   aws.s3::s3read_using(
#   FUN = data.table::fread,
#   encoding = "UTF-8",
#   object = "data_tidy/BDD_PA_FAPBM.csv",
#   bucket = "projet-afd-eva-ap",
#   opts = list("region" = ""))

##Madagascar (all PAs)
# data_pa =
#   #fread("data_tidy/BDD_PA_AFD_ie.csv" , encoding = "UTF-8")
#   aws.s3::s3read_using(
#   FUN = data.table::fread,
#   encoding = "UTF-8",
#   object = "data_tidy/BDD_PA_MDG.csv",
#   bucket = "projet-afd-eva-ap",
#   opts = list("region" = ""))

# All PAs in Africa
data_pa =
  aws.s3::s3read_using(
  FUN = data.table::fread,
  encoding = "UTF-8",
  object = "data_tidy/BDD_PA_africa.csv",
  bucket = "projet-afd-eva-ap",
  opts = list("region" = ""))


#Specify the period of study to create the mapme.bidiversity portfolio
## Start year
yr_first = 2000
## End year
yr_last = 2021
## Year to start analysis
yr_min = yr_first + 2

#Specify the margin of error to define confidence interval (0.05 corresponds 95% confidence interval).
alpha = 0.05

# Criteria to assess matching quality
## Standardized absolte mean difference : threshold
th_mean = 0.25 #Used in conservation science, see Desbureaux 2021 for instance
## Variance ratio : thresholds
th_var_min = 0.5
th_var_max = 2

# The list of countries (ISO3 codes) to analyze. This can be define manually or from the the dataset loaded.
##List of African countries in the sample that have at least one PA supported by the AFD we can analyse
data_pa_ie_africa= data_pa %>%
  filter(region == "Africa" & is.na(wdpaid) == FALSE & area_km2 >=1 & marine %in% c(0,1) & status_yr >= yr_min)
data_pa_ie_africa_focus = data_pa %>%
  filter(region == "Africa" & is.na(wdpaid) == FALSE & area_km2 >=1 & marine %in% c(0,1) & status_yr >= yr_min & focus == T)

list_iso_ie_africa_focus = unique(data_pa_ie_africa_focus$iso3)
list_pa_ie_africa_focus = unique(data_pa_ie_africa_focus$wdpaid)
list_iso = list_iso_ie_africa_focus
list_iso = c("COM", "STP", "GNB", "GMB", "SEN", "MAR", "GAB", "KEN", "CMR", "COG", "MDG", "MOZ", "NER")

## Information on funding from AFD internal datasets, on AFD funded projects related to protected areas.
data_fund = 
  #fread("data_tidy/BDD_PA_AFD_fund.csv")
  aws.s3::s3read_using(
  FUN = data.table::fread,
  encoding = "UTF-8",
  # Mettre les options de FUN ici
  object = "data_tidy/BDD_PA_AFD_fund.csv",
  bucket = "projet-afd-eva-ap",
  opts = list("region" = ""))
data_fund_nodupl = data_fund %>%
  group_by(id_projet) %>%
  slice(1) %>%
  ungroup() %>%
  dplyr::select(-c("mt_part_cofinancier_prevu_euro", "cofinanciers_siop")) %>%
  mutate(kfw = grepl("kfw", cofinanciers, ignore.case = TRUE),
         ffem = grepl("ffem", cofinanciers, ignore.case = TRUE))

## List of projects related to protected areas in AFD, reported by technical departments
data_pa_report = 
  #read_delim("data_raw/BDD_PA_AFD.csv", delim = ";")
  s3read_using(readr::read_delim,
                delim = ";",
               show_col_types = FALSE,
                object = "data_raw/BDD_PA_AFD.csv",
                bucket = "projet-afd-eva-ap",
                opts = list("region" = "")) %>%
  mutate(key = paste(id_projet, nom_ap, wdpaid, sep = "_")) %>%
  group_by(key) %>%
  slice(1)



```

## Computing treatment effects at protected area level

```{r, eval = F}
#For each country in the list, the different steps of the processing are performed
count_i = 0 #Initialize counter
max_i = length(list_iso) #Max value of the counter
tic = tic() #Start timer
#Initialize a dataframe to store the PA in inputs and outputs of the DiD. Useful to assess the potential loss during post-processing and DiD
df_list_did_in = data.frame()
df_list_did_out = data.frame()
#Create empty dataframes that will store the treatment effects computed for each protected area in the portfolio. 
##Initialize a dataframe to store annual deforestation rate for each PA, à la Wolf et al. 2021
#df_fl_annual_wolf = data.frame()
##Initialize a dataframe to store treatment effects of all PA analyzed
df_fc_att = data.frame() #Effect on forest cover for matched units
##df_fc_att_unm = data.frame() #Effect on forest cover for unmacthed units
df_fl_att = data.frame() #Effect on deforestation relative to 2000
##Initialize a dataframe to store forest loss for visual evidence before/after matching
df_plot_forest_loss = data.frame()
#Initialize a dataframe to store pre-test of parallel trend assumption
df_pre_test = data.frame()

for (i in list_iso)
{
  #Update counter and show progress
  count_i = count_i+1
  print(paste0(i, " : country ", count_i, "/", max_i))
  
  #Initialize a dataframe to store treatment effects and pre-did dataframe of each PA in the country. Used later to aggregate results at country level
  df_fc_att_i = data.frame()
  df_fl_att_i = data.frame()
  df_pre_test_i = data.frame()
  
  #Load the matching frame
  print("--Loading the list of PAs in the country considered")
  
  output_pa_i = fn_did_list_pa(iso = i, load_dir = load_dir)
  if(output_pa_i$is_ok == FALSE) {next} else list_pa_in = output_pa_i$list_pa
  
  df_list_did_in = rbind(df_list_did_in, data.frame("iso3" = rep(i, length(list_pa_in)),
                                                   "wdpaid" = list_pa_in))
  
  count_j = 0
  max_j = length(list_pa_in)
  
  for(j in list_pa_in)
  {
    
    count_j = count_j+1
    print(paste0("WDPA ID ", j, " : ", count_j, "/", max_j))
    
    #Compute annual deforestation rates in treated and control matched areas, à la Wolf et al. 2021.
    # print("--Compute average deforestation rates à la Wolf et al. 2021")
    # output_wolf_m_j = fn_fl_wolf(iso = i, 
    #                             wdpaid = j, 
    #                             alpha = alpha, 
    #                             load_dir = load_dir)
    # 
    # if(output_wolf_m_j$is_ok == FALSE) 
    #   {next} else df_fl_wolf_m_j = output_wolf_m_j$df_fl_annual_wolf
    # df_fl_annual_wolf = rbind(df_fl_annual_wolf, df_fl_wolf_m_j)
    
    #Compute treatment effects
    print("--Compute treatment effects")
    print("----For matched units")
    
    #For AFD projects (funding info)
    # output_att_m_j = fn_did_att_afd(iso = i, wdpaid = j, 
    #                       is_m = TRUE,
    #                   data_pa = data_pa,
    #                   data_fund = data_fund_nodupl,
    #                   data_report = data_pa_report,
    #                   alpha = alpha,
    #                   load_dir = load_dir,
    #                   save_dir = save_dir)
    
    #For PAs in general (no funding info)
    output_att_m_j = fn_did_att_general(iso = i, wdpaid = j, 
                        is_m = TRUE,
                    data_pa = data_pa,
                    alpha = alpha,
                    load_dir = load_dir,
                    save_dir = save_dir)
    
    if(output_att_m_j$is_ok == FALSE) {next} 
    
    df_fc_att_i = rbind(df_fc_att_i, output_att_m_j$df_fc_att)
    df_fl_att_i = rbind(df_fl_att_i, output_att_m_j$df_fl_att)
    df_pre_test_i = rbind(df_pre_test_i, output_att_m_j$df_pre_test)
        
    #print("----For unmatched units")
    
    #For AFD PAs (funding info known)
    # df_att_unm_j = fn_did_att_afd(iso = i, wdpaid = j, 
    #                       is_m = FALSE,
    #                   data_pa = data_pa,
    #                   alpha = 0.05,
    #                   load_dir = load_dir,
    #                   ext_input = ".csv",
    #                   save_dir = save_dir)
    #
    #For general PAs (funding info unknown)
    # df_att_unm_j = fn_did_att_general(iso = i, wdpaid = j, 
    #                       is_m = FALSE,
    #                   data_pa = data_pa,
    #                   alpha = 0.05,
    #                   load_dir = load_dir,
    #                   ext_input = ".csv",
    #                   save_dir = save_dir)
    #
    #if(output_att_m_j$is_ok == FALSE) {next} else df_att_m_j = output_att_m_j
    #df_pre_test_i = rbind(df_pre_test, df_att_m_j$df_pre_test)
    # df_fc_att_unm_i = rbind(df_fc_att_i, df_att_unm_j$df_fc_att)
    # df_fl_att_unm_i = rbind(df_fl_att_i, df_att_unm_j$df_fl_att)
    
    #Plot visual evidence before-after matching
    print("--Plot visual evidence before-after matching")
    df_plot_forest_loss_j = fn_plot_forest_loss(iso = i,
                                                wdpaid = j,
                                                alpha = alpha,
                                                data_pa = data_pa,
                                                load_dir = load_dir,
                                                save_dir = save_dir)
    df_plot_forest_loss = rbind(df_plot_forest_loss, df_plot_forest_loss_j)
    
    #The PA has gone through all DiD steps : report it in the output dataframe
    df_list_did_out = rbind(df_list_did_out, data.frame("iso3" = i, "wdpaid" = j))

  }  
  #Store the treatment effects dataframes of the country
  ##Report country results in the dataframe
  ###Forest cover
  df_fc_att = rbind(df_fc_att, df_fc_att_i)
  ###Forest loss
  df_fl_att = rbind(df_fl_att, df_fl_att_i)
  ##Save country results in the storage
  ###Forest cover
  aws.s3::s3write_using(
  FUN = data.table::fwrite,
  df_fc_att_i,
  object = paste(save_dir, i, paste0("df_fc_att_", i, ".csv"), sep = "/"),
  bucket = "projet-afd-eva-ap",
  opts = list("region" = ""))
  ###Forest loss
  aws.s3::s3write_using(
  FUN = data.table::fwrite,
  df_fl_att_i,
  object = paste(save_dir, i, paste0("df_fl_att_", i, ".csv"), sep = "/"),
  bucket = "projet-afd-eva-ap",
  opts = list("region" = ""))
  
  ##Store the pre-did dataframe of the country 
  ##Report country results in the dataframe
  df_pre_test = rbind(df_pre_test, df_pre_test_i)
  ##Save country results in the storage
  aws.s3::s3write_using(
  FUN = data.table::fwrite,
  df_pre_test_i,
  object = paste(save_dir, i, paste0("df_pre_test_", i, ".csv"), sep = "/"),
  bucket = "projet-afd-eva-ap",
  opts = list("region" = ""))
  
}

#Save list of PA in input and output of DiD process
##Input
aws.s3::s3write_using(
FUN = data.table::fwrite,
df_list_did_in,
object = paste(save_dir, "df_list_did_in.csv", sep = "/"),
bucket = "projet-afd-eva-ap",
opts = list("region" = ""))
##Output
aws.s3::s3write_using(
FUN = data.table::fwrite,
df_list_did_out,
object = paste(save_dir, "df_list_did_out.csv", sep = "/"),
bucket = "projet-afd-eva-ap",
opts = list("region" = ""))

#Save the pre-treatment tests 
aws.s3::s3write_using(
FUN = data.table::fwrite,
df_pre_test,
object = paste(save_dir, "df_pre_test.csv", sep = "/"),
bucket = "projet-afd-eva-ap",
opts = list("region" = ""))

#Save the treatment effects computed for every protected areas analyzed
## Treatment effect expressed in forest cover loss avoided (ha)
aws.s3::s3write_using(
FUN = data.table::fwrite,
df_fc_att,
object = paste(save_dir, "df_fc_att.csv", sep = "/"),
bucket = "projet-afd-eva-ap",
opts = list("region" = ""))
## Treatment effect expressed in change of deforestation rate (percentage points)
aws.s3::s3write_using(
FUN = data.table::fwrite,
df_fl_att,
object = paste(save_dir, "df_fl_att.csv", sep = "/"), 
bucket = "projet-afd-eva-ap",
opts = list("region" = ""))

#Save forest loss on the 2000-2021 period computed for each PA
aws.s3::s3write_using(
FUN = data.table::fwrite,
df_plot_forest_loss,
object = paste(save_dir, "df_fl_2000_2021.csv", sep = "/"), 
bucket = "projet-afd-eva-ap",
opts = list("region" = ""))

toc = toc()


```

## Assess the quality of matching and pre-test

The treatment effects are computed using a difference-in-difference framework where control are built from a matching procedure. The treatment effects computed can be biased if (1) matched control and treated units are too different on average; (2) matched and unmatched treated units are too different on average (Schleicher et al., 2020). Also, the estimation from difference-in-difference is biased if parallel trend assumption does not hold, which is likely if control and treated matched unit do no follow the same trend before treatment.

In matching and DiD procedures, we have computed for each PA some statistics to assess the quality of the matching process and the pre-treatment parallel trend assumption. Before plotting and aggregating treatment effects, we first remove a PA if these statistics suggest poor matching of pre-treatment parallel trend.

```{r, eval = FALSE}

#Import the quality assessment datasets
## Pre-treatment parallel trend assumption test
df_pre_test = aws.s3::s3read_using(
  FUN = data.table::fread,
  object = paste(save_dir, "df_pre_test.csv", sep = "/"),
  bucket = "projet-afd-eva-ap",
  opts = list("region" = "")) %>%
  mutate(is_ok_fc = pval_fc <= alpha,
         is_ok_fl = pval_fl <= alpha)
##Matching quality
##Matched control and treated units
df_quality_ct = aws.s3::s3read_using(
FUN = data.table::fread,
object = paste(load_dir, "df_quality_ct.csv", sep = "/"),
bucket = "projet-afd-eva-ap",
opts = list("region" = ""))
##Treated units, before and after matching
df_quality_tt = aws.s3::s3read_using(
FUN = data.table::fread,
object = paste(load_dir, "df_quality_tt.csv", sep = "/"), 
bucket = "projet-afd-eva-ap",
opts = list("region" = ""))

#For matching quality, compute aggregated quality metrics
##For all PA
df_quality_ct_agg = df_quality_ct %>%
  group_by(region, country_en, iso3,wdpaid) %>%
  summarise(std_mean_diff_agg = mean(std_mean_diff, na.rm = TRUE),
            var_ratio_agg = mean(var_ratio, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(is_mean_ok = std_mean_diff_agg <= th_mean,
         is_var_ok = var_ratio_agg >= th_var_min & var_ratio_agg <= th_var_max,
         is_ok = case_when(is.na(is_mean_ok*is_var_ok) == T ~ FALSE,
                           is.na(is_mean_ok*is_var_ok) == F ~ as.logical(is_mean_ok*is_var_ok)))
df_quality_tt_agg = df_quality_tt %>%
  group_by(region, country_en, iso3,wdpaid) %>%
  summarise(std_mean_diff_agg = mean(std_mean_diff, na.rm = TRUE),
            var_ratio_agg = mean(var_ratio, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(is_mean_ok = std_mean_diff_agg <= th_mean,
         is_var_ok = var_ratio_agg >= th_var_min & var_ratio_agg <= th_var_max,
         is_ok = case_when(is.na(is_mean_ok*is_var_ok) == T ~ FALSE,
                           is.na(is_mean_ok*is_var_ok) == F ~ as.logical(is_mean_ok*is_var_ok)))

##For PA of interest
df_quality_ct_agg_focus = df_quality_ct_agg %>%
  filter(wdpaid %in% list_pa_ie_africa_focus)
df_quality_tt_agg_focus = df_quality_tt_agg %>%
  filter(wdpaid %in% list_pa_ie_africa_focus)

#List of PA with acceptable matching and pre-test (strict criterion)
list_pa_good_ct = df_quality_ct_agg[df_quality_ct_agg$is_ok == TRUE,]$wdpaid
list_pa_good_tt = df_quality_tt_agg[df_quality_tt_agg$is_ok == TRUE,]$wdpaid
list_pa_good_pre_fc = df_pre_test[df_pre_test$is_ok_fc == TRUE,]$wdpaid
list_pa_good_pre_fl = df_pre_test[df_pre_test$is_ok_fl == TRUE,]$wdpaid
list_pa_keep_fc = data_pa[data_pa$wdpaid %in% list_pa_good_ct
                       & data_pa$wdpaid %in% list_pa_good_tt
                       & data_pa$wdpaid %in% list_pa_good_pre_fc,]$wdpaid
```

## Assess the loss of observations

Some PA can be dropped during the matching and DiD process for several reasons : the PA without overlap can be too small compared to the initial area, no observation unit in a given PA is matched, matching quality is poor, pre-test does not hold. In any communication, it is necessary to be explicit about such losses. In both matching and DiD processes, dataframe are built to report the PAs in the different steps : before and after pre-processing, post-processing and DiD.

```{r, eval = F}

#Import the datasets
## Input to post-processing (=output of pre-processing) -> losses due to remove of PA overlap
df_list_post_in = aws.s3::s3read_using(
FUN = data.table::fread,
object = paste(load_dir, "df_list_post_in.csv", sep = "/"),
bucket = "projet-afd-eva-ap",
opts = list("region" = ""))
## Output of post-processing -> losses due to absence of matching
df_list_post_out = aws.s3::s3read_using(
FUN = data.table::fread,
object = paste(load_dir, "df_list_post_out.csv", sep = "/"),
bucket = "projet-afd-eva-ap",
opts = list("region" = ""))
## Input of DiD : equivalent to output of post-processing a priori
df_list_did_in = aws.s3::s3read_using(
FUN = data.table::fread,
object = paste(save_dir, "df_did_list_in.csv", sep = "/"),
bucket = "projet-afd-eva-ap",
opts = list("region" = ""))
## Output of DiD : losses due to a too small never treated group to compute DiD
df_list_did_out = aws.s3::s3read_using(
FUN = data.table::fread,
object = paste(save_dir, "df_did_list_out.csv", sep = "/"),
bucket = "projet-afd-eva-ap",
opts = list("region" = ""))

#Compute losses
n_ini = nrow(data_pa) #Number of PA in Africa
n_ini_focus = nrow(filter(data_pa, focus == T)) #number of AFD supported PA in Africa
n = nrow(filter(data_pa, iso3 %in% list_iso_ie_africa_focus)) #Number of PA in the African countries with at least one AFD supported PA we can analyse
n_focus = nrow(filter(data_pa, iso3 %in% list_iso_ie_africa_focus & focus == T)) #Number of AFD supported PA we can analyse
n_pre = nrow(df_list_post_in) #Number of PA after pre-processing
n_pre_focus = nrow(filter(df_list_post_in, wdpaid %in% list_pa_ie_africa_focus)) #Number of AFD supported PA after pre-processing
n_post = nrow(df_list_post_out) #Number of PA after post-processing
n_post_focus = nrow(filter(df_list_post_out, wdpaid %in% list_pa_ie_africa_focus)) #Number of AFD supported PA after post-processing
n_did = nrow(df_list_did_out) #Number of PA after post-processing
n_did_focus = nrow(filter(df_list_did_out, wdpaid %in% list_pa_ie_africa_focus)) #Number of AFD supported PA after post-processing
```

## Compute aggregated metrics at country and region level

The aggregation of results at country and regional level is not necessarily relevant when the number of protected areas by country or region is relatively small and results heterogenous across protected areas. Instead, a figure displaying the results for all protected areas in the sample might be mor relevant. See next section.

### Aggregation of treatment effects

```{r, eval = FALSE}

#Import datasets
## Treatment effect expressed in forest cover loss avoided (ha)
df_fc_att = aws.s3::s3read_using(
FUN = data.table::fread,
object = paste(save_dir, "df_fc_att.csv", sep = "/"),
bucket = "projet-afd-eva-ap",
opts = list("region" = ""))
## Treatment effect expressed in change of deforestation rate (percentage points)
df_fl_att = aws.s3::s3read_using(
FUN = data.table::fread,
object = paste(save_dir, "df_fl_att.csv", sep = "/"),
bucket = "projet-afd-eva-ap",
opts = list("region" = ""))

#Call the function to aggregate results at country level and save plots
## For PA of interest
fn_plot_att_agg(df_fc_att = df_fc_att,
                df_fl_att = df_fl_att,
                is_focus = T,
                alpha = alpha,
                save_dir = save_dir)
# For other PA
fn_plot_att_agg(df_fc_att = df_fc_att,
                df_fl_att = df_fl_att,
                is_focus = F,
                alpha = alpha,
                save_dir = save_dir)
```

### Aggregation of annual deforestation rates

```{r, eval = F}

#Aggregate at region level
## Compute for each region, for treated units, average annual deforestation rate before treatment, after treatment and on the full period. Confidence intervals are also computed.
# avg_fl_annual_wolf_region = df_fl_annual_wolf %>%
#   filter(group == "Treated") %>%
#   group_by(region) %>%
#   summarize(avg_FL_annual_wolf_pre = mean(avgFL_annual_wolf_pre, na.rm = TRUE),
#             avg_FL_annual_wolf_pre_ci_up = mean(avgFL_annual_wolf_pre, na.rm = TRUE) +  qt((1-alpha)/2,df=21-1)*sd(avgFL_annual_wolf_pre, na.rm = TRUE)/sqrt(21),
#             avg_FL_annual_wolf_pre_ci_lower = mean(avgFL_annual_wolf_pre, na.rm = TRUE) -  qt((1-alpha)/2,df=21-1)*sd(avgFL_annual_wolf_pre, na.rm = TRUE)/sqrt(21),
#             med_FL_annual_wolf_pre_ci_up = median(avgFL_annual_wolf_pre, na.rm = TRUE),
#             avg_FL_annual_wolf_tot = mean(avgFL_annual_wolf_tot, na.rm = TRUE),
#             avg_FL_annual_wolf_tot_ci_up = mean(avgFL_annual_wolf_tot, na.rm = TRUE) +  qt((1-alpha)/2,df=21-1)*sd(avgFL_annual_wolf_tot, na.rm = TRUE)/sqrt(21),
#             avg_FL_annual_wolf_tot_ci_lower = mean(avgFL_annual_wolf_tot, na.rm = TRUE) -  qt((1-alpha)/2,df=21-1)*sd(avgFL_annual_wolf_tot, na.rm = TRUE)/sqrt(21),
#             med_FL_annual_wolf_tot_ci_up = median(avgFL_annual_wolf_tot, na.rm = TRUE)) 

#Aggregate at country level
## Same that at region level.
# avg_fl_annual_wolf_country = df_fl_annual_wolf %>%
#   filter(group == "Treated") %>%
#   group_by(iso3) %>%
#   summarize(avg_FL_annual_wolf_pre = mean(avgFL_annual_wolf_pre, na.rm = TRUE),
#             avg_FL_annual_wolf_pre_ci_up = mean(avgFL_annual_wolf_pre, na.rm = TRUE) +  qt((1-alpha)/2,df=21-1)*sd(avgFL_annual_wolf_pre, na.rm = TRUE)/sqrt(21),
#             avg_FL_annual_wolf_pre_ci_lower = mean(avgFL_annual_wolf_pre, na.rm = TRUE) -  qt((1-alpha)/2,df=21-1)*sd(avgFL_annual_wolf_pre, na.rm = TRUE)/sqrt(21),
#             med_FL_annual_wolf_pre_ci_up = median(avgFL_annual_wolf_pre, na.rm = TRUE),
#             avg_FL_annual_wolf_tot = mean(avgFL_annual_wolf_tot, na.rm = TRUE),
#             avg_FL_annual_wolf_tot_ci_up = mean(avgFL_annual_wolf_tot, na.rm = TRUE) +  qt((1-alpha)/2,df=21-1)*sd(avgFL_annual_wolf_tot, na.rm = TRUE)/sqrt(21),
#             avg_FL_annual_wolf_tot_ci_lower = mean(avgFL_annual_wolf_tot, na.rm = TRUE) -  qt((1-alpha)/2,df=21-1)*sd(avgFL_annual_wolf_tot, na.rm = TRUE)/sqrt(21),
#             med_FL_annual_wolf_tot_ci_up = median(avgFL_annual_wolf_tot, na.rm = TRUE)) %>%
#   ungroup() 
  

```

### Average and total forest loss on the period, at country and region level

For each protected area, the total deforestation is estimated on the period in the protected area and a counterfactual, before and after matching. This metric can be aggregated at country and region level : the sum of total deforestation or its average. This can be computed for all protected areas in the sample, a specific subset of protected areas, but also at country or region level.

```{r, eval = F}

#Import datasets
df_plot_forest_loss = aws.s3::s3read_using(
FUN = data.table::fread,
object = paste(save_dir, "df_fl_2000_2021.csv", sep = "/"),
bucket = "projet-afd-eva-ap",
opts = list("region" = ""))

#Call a fcuntion to plot total and average forest loss on the 2000-2021 period, for PA of interest and others

fn_plot_forest_loss_agg(df_plot_forest_loss = df_plot_forest_loss,
                        alpha = alpha,
                        save_dir = save_dir)


```

## Display treatment effects in figures and tables

The treatment effects computed for each protected areas can be displayed in figures or tables. Again, a function is used for protected areas supported by the AFD to include information on funding (funding year for instance), and an other for non-supported ones.

Note some protected areas can be removed from the display, because the matching is not satisfying or pre-treatment trends are too different between treated and control units (thus the matched control risk to be an irrelevant counterfactual). Also, one might want to highlight some protected areas. For instance when comparing a subset of protected areas in a given country to the others (protected areas in Madagascar supported by the AFD versus the others for instance).

```{r, eval = F}
# Define a list of protected areas that will not be displayed
list_wdpa_bad = c()
# Define a list of protected areas where a focus is needed
list_wdpa_focus = list_pa_ie_africa_focus

# Get rid of some protected areas in the dataset used to plotting figures and tables
df_fc_att_tidy = df_fc_att %>%
  filter(!wdpaid %in% list_wdpa_bad)
df_fl_att_tidy = df_fl_att %>%
  filter(!wdpaid %in% list_wdpa_bad)

# A function to create figures and tables
##For AFD supported protected areas (funding info)
# fn_plot_att_afd(df_fc_att = df_fc_att_tidy,
#             df_fl_att = df_fl_att_tidy, 
#             alpha = alpha,
#             save_dir = save_dir)
##For other protected areas (no funding info)
fn_plot_att_general(df_fc_att = df_fc_att_tidy,
            df_fl_att = df_fl_att_tidy, 
            list_focus = list_wdpa_focus,
            alpha = alpha,
            save_dir = save_dir)

```

## 
