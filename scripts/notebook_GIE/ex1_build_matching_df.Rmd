---
title: "Exercise_1"
output: html_document
date: "2024-07-23"
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The following exercises will help you to master the steps of the
analysis carried out in the chapter "Geospatial Impact Assessment of
Conservation on Forest Cover Loss" and to understand the methodology
used. The evaluation is conducted here for a single protected area to
make it easier to understand the code, but the scripts for analysing the
portfolio can be found on the 'github link'.

## Exercise 1 : Build a matching sample

```{r}
# install packages if necessary
lop <- c("ggplot2", "tidyr", "dplyr", "stringr", "sf", "terra", "raster", "geodata", "exactextractr", "mapme.biodiversity", "future","progressr","wdpar","landscapemetrics","gridExtra","rstac")
newp <- lop[!(lop %in% installed.packages()[,"Package"])]
if(length(newp)) install.packages(newp)
lapply(lop, require, character.only = TRUE)
```

```{r }
# Load Libraries
library(dplyr) # A grammar of data manipulation
library(tidyr) # Tools for creating tidy data
library(stringr) # String manipulation functions
library(ggplot2) # For plotting
library(sf) # For handling vector data
library(terra) # For handling raster data
library(raster) # For handling raster data
library(geodata) # For getting country files
remotes::install_github("prioritizr/wdpar@extra-wait") # to download protected areas shape
webdriver::install_phantomjs() # to fetch wdpa data 
library(exactextractr) # For zonal statistics
remotes::install_github("mapme-initiative/mapme.biodiversity", upgrade="always")
library(mapme.biodiversity) # Biodiversity data processing and analysis
library(future) # For parallel and distributed processing
library(progressr) # Progress bars for long-running operations
library(gridExtra)
library(rstac)
```

## 1. Settings ⚙️

The first step is to decide the parameters of your analysis, in the next
cell you have to enter: 
- a working directory 
- the name of the output
matching frame 
- the country code of El Salvador - the size of the
buffer - the size of the grid - the WDPA ID of the Parque Nacional
Montecristo

To lower the computation time we choose to analyse the Parque Nacional
Montecristo but you can try with another PA.

```{r}
# Define the path to a working directory
wdir = file.path(tempdir())
# Define the file name of the output matching frame
name_output = "mf_SLV_500ha.gpkg"
# Specify a country iso 3; to find the right country code, please refer to this page https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3
country = "SLV"
# Specify buffer width in meter
buffer_m = 10000
# Specify the grid cell size in meter
gridSize = 2236.068 # --> 500ha
# Specify a WDPA IDs; to find the WDPA ID, please refer to this page 
# https://www.protectedplanet.net/en
paid = c(9638)
```

## 2. Generating observation units for matching frame

The second step is to prepare a grid of observation units for the
spatial analysis, tailored specifically to the chosen country. After
downloading the country polygon, it is necessary to reproject the
country polygon to the appropriate UTM (Universal Transverse Mercator)
zone based on the country's centroid. The UTM projection minimizes
distortion for specific regions, making spatial calculations like
distances and areas more accurate. A bounding box is created, and a grid
is generated within this box, intersecting with the country's boundary
to form the observation units.

[reference to the chapter about projection ]

1.  [ADD A QUESTION ABOUT PROJECTION] ASK MELVIN 


2.  Add the right objects in the \# Visualize section to obtain a map
    with the box,the grid and the shape of the country

```{r}
# Download country polygon to working directory and load it into workspace 
gadm <- gadm(country = country, resolution = 1, level = 0, path = wdir) %>%
  st_as_sf()

# Find UTM zone of the country centroid
centroid = st_coordinates(st_centroid(gadm))
lonlat2UTM = function(lonlat) {
  utm = (floor((lonlat[1] + 180) / 6) %% 60) + 1
  if (lonlat[2] > 0) {
    utm + 32600
  } else{
    utm + 32700
  }
}
utm_code = lonlat2UTM(centroid)
# Reproject GADM
gadm_prj = gadm %>% st_transform(crs = utm_code)
# Make bounding box of projected country polygon
bbox = st_bbox(gadm_prj) %>% st_as_sfc() %>% st_as_sf()
# Make a Grid to the extent of the bounding box
grid.ini = st_make_grid(bbox, cellsize = c(gridSize,gridSize))
# Crop Grid to the extent of country boundary by
# subsetting to the grid cells that intersect with the country
grid.sub = grid.ini %>%
  st_intersects(gadm_prj, .) %>%
  unlist()
# Filter the grid to the subset
grid = grid.ini[sort(grid.sub)] %>%
  st_as_sf() %>%
  mutate(gridID = seq(1:nrow(.))) # Add id for grid cells
# Visualize
ggplot() +
 geom_sf(data = st_geometry(bbox)) +
 geom_sf(data = st_geometry(gadm_prj)) +
 geom_sf(data = st_geometry(grid), alpha = 0)

```

## 3. Determining Group IDs and WDPA IDs for all observation units

Then, the code works with protected area (PA) polygons, separating
funded and non-funded PAs and adding buffer zones around them. These PA
polygons are filtered, cleaned, and projected to the UTM zone, then
merged into a single dataset with group identifiers for funded,
non-funded, and buffer areas.

1.Find the right sf function to create buffer around all PA using the
buffer size (buffer_m) chosen before and complete the code

```{r}
# Get the PA polygons/points of the specified country;
# They're downloaded to the working directory.
wdpa = wdpa_fetch(country, wait = TRUE, download_dir = wdir)

# If the PA file already exists, it can be loaded in this way
#wdpa = wdpa_read(paste0(wdir, '/WDPA_Jul2023_BOL-shapefile.zip'))

# PAs are projected, and column "geometry_type" is added
wdpa_prj = wdpa_clean(wdpa, geometry_precision = 1000,
                      # Don't erase overlapped polygons
                      erase_overlaps = FALSE) %>%
  # Remove the PAs that are only proposed, or have geometry type "point"
  filter(STATUS != "Proposed") %>%
  filter(GEOMETRY_TYPE != "POINT") %>%
  # Project PA polygons to the previously determined UTM zone
  st_transform(crs = utm_code)

# Separate funded and non-funded protected areas
wdpaID_funded = paid
wdpa_funded = wdpa_prj %>% filter(WDPAID %in% wdpaID_funded) %>%
  mutate(group=1) # Assign an ID "1" to the funded PA group

wdpa_nofund = wdpa_prj %>%
  filter(!WDPAID %in% wdpaID_funded) %>%
  st_buffer(., dist=0) # a hack to solve geometries issues with polygons in R, e.g. problem of self-intersection
#add a link 

# Determine the non-funded PAs that intersect with funded PAs,
# and delete the intersection part from non-funded PAs to reduce noise when rasterizing WDPAIDs later.

intersection = st_intersection(wdpa_nofund, wdpa_funded)
wdpa_nofund <- wdpa_nofund %>%
  { if (nrow(intersection) == 0) . else st_difference(., st_union(st_geometry(intersection))) } %>%
  mutate(group = 2)

# Make Buffers of 10km around all protected areas
# Complete this section to make buffer around wdpa_funded and wdpa_nofund PAs and then bind them together 
wdpa_buffer <- st_buffer(wdpa_funded, dist = buffer_m) %>%
  rbind(st_buffer(wdpa_nofund, dist = buffer_m)) %>%
  # Assign an ID "3" to the buffer group
  mutate(group=3)

# Merge the dataframes of funded PAs, non-funded PAs and buffers
wdpa_groups = rbind(wdpa_funded, wdpa_nofund, wdpa_buffer)
# Subset to polygons that intersect with country boundary
wdpa.sub = wdpa_groups %>%
  st_intersects(gadm_prj, .) %>%
  unlist()
# Filter the PA+buffer to the subset
wdpa_groups = wdpa_groups[sort(wdpa.sub),] %>%
  st_as_sf()



# This map allow to visualize each PA in the country and its buffer zone around
colors <- c("1" = "darkgreen", "2" = "darkblue", "3" = "orange")

ggplot() +
   geom_sf(data = st_geometry(gadm_prj), fill = "lightgray", color = "lightgray", lwd = 0.2) + 
    geom_sf(data = subset(wdpa_groups, group == "3"), aes(fill = factor(group)), color = "black", lwd = 0.3, alpha = 0.6) +
    geom_sf(data = subset(wdpa_groups, group == "2"), aes(fill = factor(group)), color = "black", lwd = 0.3, alpha = 0.6) +
    geom_sf(data = subset(wdpa_groups, group == "1"), aes(fill = factor(group)), color = "black", lwd = 0.3, alpha = 0.6) +
    scale_fill_manual(values = colors, 
                      labels = c("1" = "Funded PA", 
                               "2" = "Non Funded PA", 
                               "3" = "Buffer Zone")) +
    theme_bw() +
    theme(legend.position = "right")


```

The next section attribute a group to each pixel of the grid. To speed
up calculations a raster grid is used.

raster

:   A **raster** is a type of spatial data representation that consists
    of a grid of pixels (or cells), where each pixel has a specific
    value representing information, such as color in an image or a
    particular data value in a geographic context

A raster grid is initialized over the country's extent, with pixel
values representing group IDs. It assigns minimal values to pixels
covered by overlapping polygons, ensuring that PA group IDs take
precedence over buffer IDs. Another raster layer is created for WDPA
IDs.

Finally, the raster values are aggregated into the grid cells with the "mode"
function,that input the value of the group with the largest presence to each pixel.
Then values get merged into a single dataset, and transformed to the WGS84
coordinate system to comply with the mapme.biodiversity package
requirements.

```{r}
# Initialize an empty raster to the spatial extent of the country
r.ini = raster()
extent(r.ini) = extent(gadm_prj)
# Specify the raster resolution as same as the pre-defined 'gridSize'
res(r.ini) = gridSize
# Assign the raster pixels with "Group" values,
# Take the minimal value if a pixel is covered by overlapped polygons, so that PA Group ID has higher priority than Buffer ID.
# Assign value "0" to the background pixels (control candidates group)
r.group = rasterize(wdpa_groups, r.ini, field="group", fun="min", background=0) %>%
  mask(., gadm_prj)
# Rename Layer
names(r.group) = "group"
# Rasterize wdpaid
r.wdpaid = rasterize(wdpa_prj, r.ini, field="WDPAID", fun="first", background=0) %>%
  mask(., gadm_prj)
names(r.wdpaid) = "wdpaid"

# Aggregate pixel values by taking the majority
grid.group = exact_extract(x=r.group, y=grid, fun='mode', append_cols="gridID") %>%
  rename(group = mode)
grid.wdpaid = exact_extract(x=r.wdpaid, y=grid, fun="mode", append_cols="gridID") %>%
  rename(wdpaid = mode)
# Merge data frames
grid.param = grid.group %>%
  merge(., grid.wdpaid, by="gridID") %>%
  merge(., grid, by="gridID") %>%
  # drop rows having "NA" in column "group"
  drop_na(group) %>%
  # drop the column of "gridID"
  subset(., select=-c(gridID)) %>%
  st_as_sf() %>%
  # Grid is projected to WGS84 because mapme.biodiverty package merely works with this CRS
  st_transform(crs=4326)
```

You can now visualize the gridding of the country and the attribution of
group to each cell.

```{r}
# Visualize grouped grid cells
grid.param %>%
  ggplot() +
  geom_sf(aes(fill = factor(group)), lwd=0) + # add the right dataframe in factor()
  scale_fill_manual(name="Group", # legend title
                    values = c("grey", "darkgreen", "darkblue", "orange"),#add colors
                    labels = c("control candidate", "treatment candidate", "non-funded PA", "buffer zone")) +
  theme_bw()

```

The function used to aggregate pixel values influence the final value
obtained.
In this script, the "mode" function is used, but other functions such as "max" or "min" could be employed instead. 
1. If, for example, a cell is to be placed in the treatment group as soon as it interferes with an AP, which function should be used?   
2. Add this function in exactextract() 
3.Compute summary statistic (distribution) for the "group" variable in grid.param and grid.param_2. Comment on the
    difference.
3.  Map grid.param and grid.param_2 to visualize the difference.

```{r}
# Aggregate pixel values by taking the min
grid.group_2 = exact_extract(x=r.group, y=grid, fun='min', append_cols="gridID") %>%
  rename(group = min)
grid.wdpaid_2 = exact_extract(x=r.wdpaid, y=grid, fun="min", append_cols="gridID") %>%
  rename(wdpaid = min)
# Merge data frames
grid.param_2 = grid.group_2 %>%
  merge(., grid.wdpaid_2, by="gridID") %>%
  merge(., grid, by="gridID") %>%
  # drop rows having "NA" in column "group"
  drop_na(group) %>%
  # drop the column of "gridID"
  subset(., select=-c(gridID)) %>%
  st_as_sf() %>%
  # Grid is projected to WGS84 because mapme.biodiverty package merely works with this CRS
  st_transform(crs=4326)

```

```{r}
# Compute summary statistic (distribution) for group variable in grid.param and grid.param_2
stat_1=table(grid.param$group)
stat_2= table(grid.param_2$group)
c(stat_1,stat_2)
```

```{r}
# Visualize maps with "mode" and "min" function
# create the graph of grid.param
map_1=grid.param %>%
  ggplot() +
  geom_sf(aes(fill = factor(group)), lwd=0) + 
  scale_fill_manual(name="Group",
                    values = c("grey", "darkgreen", "darkblue", "orange"),
                    labels = c("control candidate", "treatment candidate", "non-funded PA", "buffer zone")) +
  theme_bw()

# create the graphe of grid.param_2
map_2=grid.param_2 %>%
  ggplot() +
  geom_sf(aes(fill = factor(group)), lwd=0) + 
  scale_fill_manual(name="Group",
                    values = c("grey", "darkgreen", "darkblue", "orange"),
                    labels = c("control candidate", "treatment candidate", "non-funded PA", "buffer zone")) +
  theme_bw()

grid.arrange(map_1, map_2, nrow = 2)
```

## 4. Download the data

Once a group have been attributed to each cells, all the covariates need
to be downloaded. To do so you can use the Mapme.biodiversity package
that enable you to download multiple ressources and then compute the
right indicators. Please refer to this page
(<https://github.com/mapme-initiative/mapme.biodiversity>) to find the
function to complete the following code.

You will be ask to :

\- Download the soil data

\- Calculate elevation

\- Download travel time data and compute the indicators

This section use multisession the speed up the computing of covariates.

Multisession

:   **Multisession** refers to a type of parallel processing where
    multiple R sessions (or processes) are created to execute code
    concurrently. Each session operates independently and runs in its
    own R process, allowing for parallel execution of tasks.

For the first variable try to rerun the cell without using multisession
and compare the execution time.

```{r}
## 4. Calculating deforestation area and other covariates for all observation units
# Get input data ready for indicator calculation
      ## Version of Global Forest Cover data to consider
      list_version_gfc = mapme.biodiversity:::.available_gfw_versions() #all versions available
      version_gfc = list_version_gfc[length(list_version_gfc)] #last version considered
#years=2000:2021
mapme_options(outdir =wdir)
# aoi = init_portfolio(grid.param,
#                      years = 2000:2021,
#                      outdir = wdir,
#                      tmpdir = file.path(wdir, "tmp"),
#                      add_resources = FALSE)

years = 2000:2021
aoi=grid.param
```

### Covariate: Soil

1.  Enter the function to download soil data. Use the following
    parameters : "clay' for the layers , a depth between 0 and 5 cm, and
    the statistic "mean".

```{r}
start_time <- Sys.time()


get.soil = aoi %>% get_resources(get_soilgrids(
        layers = "clay", # resource specific argument
        depths = "0-5cm", # resource specific argument
        stats = "mean"))
                         
# set up parallel plan with 6 concurrent threads
plan(multisession, workers = 20)
# Calculate Indicator
with_progress({
  zonal.soil = get.soil %>% calc_indicators(
          calc_soilproperties(
            stats = "mean",
            engine = "zonal"
          )
        )
})
plan(sequential) # close child processes
# Transform the output dataframe into a pivot dataframe
pivot.soil = zonal.soil %>%
  unnest(soilproperties) %>%
  mutate(across(value, round, 3)) %>% # Round numeric columns
  pivot_wider(names_from = c("variable"), values_from = "value")%>%
  dplyr::select(-c(datetime,unit))
      


end_time <- Sys.time()
elapsed_time <- end_time - start_time

cat("Temps d'exécution:", elapsed_time, "\n")

```

2.  Try running the same cells without the multisession and compare

```{r}
start_time <- Sys.time()
get.soil = aoi %>% get_resources(get_soilgrids(
        layers = "clay", # resource specific argument
        depths = "0-5cm", # resource specific argument
        stats = "mean"))
                         

# Calculate Indicator
with_progress({
  zonal.soil = get.soil %>% calc_indicators(
          calc_soilproperties(
            stats = "mean",
            engine = "zonal"
          )
        )
})

# Transform the output dataframe into a pivot dataframe
pivot.soil = zonal.soil %>%
  unnest(soilproperties) %>%
  mutate(across(value, round, 3)) %>% # Round numeric columns
  pivot_wider(names_from = c("variable"), values_from = "value")%>%
  dplyr::select(-c(datetime,unit))
      


end_time <- Sys.time()
elapsed_time <- end_time - start_time

cat("execution time:", elapsed_time, "\n")
```

### Covariate: Elevation

Insert the function to calculate elevation mean, you should use the mean
statistic and the exactextract engine :

```{r}
get.elevation = aoi %>% get_resources(get_nasa_srtm())
# set up parallel plan with 6 concurrent threads
plan(multisession, workers = 20)

# Calculate Indicator
with_progress({
  zonal.elevation = get.elevation %>% calc_indicators(calc_elevation(
          stats= "mean",
          engine = "exactextract"))
  
  
})
plan(sequential) # close child processes
# Transform the output dataframe into a pivot dataframe
pivot.elevation = zonal.elevation %>% unnest(elevation)%>%
  pivot_wider(names_from = c("variable"), values_from = "value")%>%
  dplyr::select(-c(datetime,unit))
      


```

### Covariate: TRI

```{r}
# set up parallel plan with 6 concurrent threads
plan(multisession, workers = 20)
# Calculate Indicator
with_progress({
  zonal.tri = get.elevation %>% calc_indicators(calc_tri(
          stats = "mean",
          engine = "exactextract"))
})
plan(sequential) # close child processes
# Transform the output dataframe into a pivot dataframe
pivot.tri = zonal.tri %>% unnest(tri)%>%
  pivot_wider(names_from = c("variable"), values_from = "value")%>%
  dplyr::select(-c(datetime,unit))


```

### Covariate: Travel Time

Download data related to travel time and calculate the median travel
time

```{r}
#Run this, to download correctly travel data 

Sys.setenv(
  "VSI_CACHE" = "TRUE",
  "CPL_VSIL_CURL_CHUNK_SIZE" = "10485760",
  "GDAL_HTTP_MAX_RETRY" = "5",
  "GDAL_HTTP_RETRY_DELAY" = "15"
)




get.travelT = aoi%>% get_resources(get_nelson_et_al(ranges = c("5k_110mio")))

# set up parallel plan with 6 concurrent threads
plan(multisession, workers = 20)
# Calculate Indicator
with_progress({
  zonal.travelT  <-get.travelT %>% calc_indicators(calc_traveltime(
          stats = "median",
          engine = "exactextract"))
  })
plan(sequential) # close child processes
# Transform the output dataframe into a pivot dataframe
pivot.travelT = zonal.travelT %>%
  unnest(traveltime) %>%
  pivot_wider(names_from = "variable", values_from = "value")%>%
  dplyr::select(-c(datetime,unit))

```

### Time Series of Tree Cover Area

```{r}
get.tree = aoi %>%get_resources(get_gfw_treecover(version =  version_gfc),
                                     get_gfw_lossyear(version = version_gfc))
# set up parallel plan with 6 concurrent threads
plan(multisession, workers = 20)
# Calculate time series
with_progress({
  zonal.tree = get.tree %>% calc_indicators(calc_treecover_area(years=years, min_size=0.5, 
                                                                    min_cover=10))
  
})

#ask Melvin about min_cover 
plan(sequential) # close child processes

# Transform the output dataframe into a pivot dataframe
pivot.tree = zonal.tree %>%
  unnest(treecover_area) %>%
  # Transfer treecover unit to percentage
  mutate(value = round((value*1e4)/(gridSize^2)*100, 2),datetime=format(datetime, "%Y")) %>%
  pivot_wider(names_from = "datetime", values_from = "value", names_prefix = "treecover_")%>%
  dplyr::select(-c(unit,variable))

```

```{r}
# The calculation of tree loss area is performed at dataframe base
# Get the column names of tree cover time series
colnames_tree = names(pivot.tree)[startsWith(names(pivot.tree), "treecover")]
# Drop the first year
dropFirst = tail(colnames_tree, -1)
# Drop the last year
dropLast = head(colnames_tree, -1)
# Set list of new column names for tree loss time series
colnames_loss = dropFirst %>% str_split(., "_")
# Add new columns: treeloss_tn = treecover_tn - treecover_t(n-1)
for (i in 1:length(dropFirst)) {
  new_colname <- paste0("treeloss_", colnames_loss[[i]][2])
  pivot.tree[[new_colname]] <- pivot.tree[[dropFirst[i]]] - pivot.tree[[dropLast[i]]]
}

# Export Matching Frame
# Remove "geometry" column from pivot dataframes
df.tree = pivot.tree %>% mutate(x = NULL) %>% as.data.frame()
df.travelT = pivot.travelT %>% mutate(x = NULL) %>% as.data.frame()
df.soil = pivot.soil %>% mutate(x = NULL) %>% as.data.frame()
df.elevation = pivot.elevation %>% mutate(x = NULL) %>% as.data.frame()
df.tri = pivot.tri %>% mutate(x=NULL) %>% as.data.frame()
# Make a dataframe containing only "assetid" and geometry
df.geom = pivot.tree[, c("assetid", "x")] %>% as.data.frame()
# Merge all output dataframes
pivot.all = Reduce(dplyr::full_join, list(df.travelT, df.soil, df.tree, df.elevation, df.tri, df.geom)) %>%
  st_as_sf()
# Make column Group ID and WDPA ID have data type "integer"
pivot.all$group = as.integer(pivot.all$group)
pivot.all$wdpaid = as.integer(pivot.all$wdpaid)

# Export the matching frame
st_write(pivot.all, dsn = file.path(wdir, name_output), delete_dsn = TRUE)

```

# 5. Create an interactive map
 
 explication sur les shiny ap 
```{r}
# Define group colors 
contour_colors <- c(
  `0` = "grey",
  `1` = "darkgreen",
  `2` = "darkblue",
  `3` = "orange"
)

# Define UI
ui <- fluidPage(
  titlePanel("Visualisation des Pixels"),
  sidebarLayout(
    sidebarPanel(
      selectInput("param", 
                  "Choisir le paramètre à afficher :", 
                  choices = names(pivot.all)[!(names(pivot.all) %in% c("x", "group","wdpaid","assetid"))])
    ),
    mainPanel(
      plotOutput("mapPlot")
    )
  )
)

# Define server logic
server <- function(input, output) {
  
  output$mapPlot <- renderPlot({
    # Créer la carte avec ggplot2
    ggplot() +
      geom_sf(data = pivot.all, aes(fill = !!sym(input$param)), color = "black") +
      scale_fill_viridis_c() +  # Palette de couleurs pour le paramètre
      scale_color_manual(values = contour_colors) +
      theme_minimal() +
      labs(fill = input$param, title = "Carte des pixels avec contours colorés") +
      theme(legend.position = "right")
  })
}

# Run the application 
shinyApp(ui = ui, server = server)
```
