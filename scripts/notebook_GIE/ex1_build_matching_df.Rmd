---
title: "Exercise_1"
output: html_document
date: "2024-07-23"
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The following exercises will help you to master the steps of the
analysis carried out in the chapter "Geospatial Impact Assessment of
Conservation on Forest Cover Loss" and to understand the methodology
used. The evaluation is conducted here for a single protected area to
make it easier to understand the code, but the scripts for analysing the
portfolio can be found on the 'github link'.

## Exercise 1 : Build a matching sample

```{r}
# install packages if necessary
lop <- c("ggplot2", "tidyr", "dplyr", "stringr", "sf", "terra", "raster", "geodata", "exactextractr", "mapme.biodiversity", "future","progressr","wdpar","landscapemetrics")
newp <- lop[!(lop %in% installed.packages()[,"Package"])]
if(length(newp)) install.packages(newp)
lapply(lop, require, character.only = TRUE)
```

```{r }
# Load Libraries
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2) # For plotting
library(sf) # For handling vector data
library(terra) # For handling raster data
library(raster) # For handling raster data
library(geodata) # For getting country files
library(wdpar) # For getting protected areas
library(exactextractr) # For zonal statistics
remotes::install_github("mapme-initiative/mapme.biodiversity", upgrade="always")
library(mapme.biodiversity)
library(future)
library(progressr)
```

```{r eval= F, message=FALSE, warning=FALSE}
# Run this, to download correctly travel data 

Sys.setenv(
  "VSI_CACHE" = "TRUE",
  "CPL_VSIL_CURL_CHUNK_SIZE" = "10485760",
  "GDAL_HTTP_MAX_RETRY" = "5",
  "GDAL_HTTP_RETRY_DELAY" = "15"
)

```

## Settings ⚙️

The first step is to decide the parameters of your analysis, in then
next cell you have to choose: - a working directory - the country code
of El Salvador - the size of the buffer - the size of the grid - the
WDPA ID of the Parque Nacional Montecristo

To lower the computation time we choose to analyse the Parque Nacional
Montecristo but you can try with another PA.

```{r}

# Define the path to a working directory
wdir = file.path(tempdir())
# Define the file name of the output matching frame
name_output = "mf_SLV_500ha.gpkg"
# Specify a country; to find the right country code, please refer to this page https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3
country = "SLV"
# Specify buffer width in meter
buffer_m = 10000
# Specify the grid cell size in meter
gridSize = 2236.068 # --> 500ha
# Specify a WDPA IDs; to find the WDPA ID, please refer to this page 
# https://www.protectedplanet.net/en
paid = c(9638)
```

## 2. Generating observation units for matching frame

The second step is to prepare a grid of observation units for the
spatial analysis, tailored specifically to the chosen country After
downloading the country polygon, it is necessary to reproject the
country polygon to the appropriate UTM (Universal Transverse Mercator)
zone based on the country's centroid. The UTM projection minimizes
distortion for specific regions, making spatial calculations like
distances and areas more accurate. A bounding box is created, and a grid
is generated within this box, intersecting with the country's boundary
to form the observation units.

Explain the purpose of the `lonlat2UTM` function. How does it determine
the correct UTM zone for the country’s centroid?

Describe what you expect to see in the final plot produced by the
`ggplot` code.

```{r}
# Download country polygon to working directory and load it into workspace 
gadm <- gadm(country = country, resolution = 1, level = 0, path = wdir) %>%
  st_as_sf()

# Find UTM zone of the country centroid
centroid = st_coordinates(st_centroid(gadm))
lonlat2UTM = function(lonlat) {
  utm = (floor((lonlat[1] + 180) / 6) %% 60) + 1
  if (lonlat[2] > 0) {
    utm + 32600
  } else{
    utm + 32700
  }
}
utm_code = lonlat2UTM(centroid)
# Reproject GADM
gadm_prj = gadm %>% st_transform(crs = utm_code)
# Make bounding box of projected country polygon
bbox = st_bbox(gadm_prj) %>% st_as_sfc() %>% st_as_sf()
# Make a Grid to the extent of the bounding box
grid.ini = st_make_grid(bbox, cellsize = c(gridSize,gridSize))
# Crop Grid to the extent of country boundary by
# subsetting to the grid cells that intersect with the country
grid.sub = grid.ini %>%
  st_intersects(gadm_prj, .) %>%
  unlist()
# Filter the grid to the subset
grid = grid.ini[sort(grid.sub)] %>%
  st_as_sf() %>%
  mutate(gridID = seq(1:nrow(.))) # Add id for grid cells
# Visualize
ggplot() +
 geom_sf(data = st_geometry(bbox)) +
 geom_sf(data = st_geometry(gadm_prj)) +
 geom_sf(data = st_geometry(grid), alpha = 0)

```



## 3. Determining Group IDs and WDPA IDs for all observation units

Then, the code works with protected area (PA) polygons, separating
funded and non-funded PAs and adding buffer zones around them. These PA
polygons are filtered, cleaned, and projected to the UTM zone, then
merged into a single dataset with group identifiers for funded,
non-funded, and buffer areas.

```{r}
# Get the PA polygons/points of the specified country;
# They're downloaded to the working directory.

#wdpa = wdpa_fetch(country, wait = TRUE, download_dir = wdir)
# If the PA file already exists, it can be loaded in this way
#wdpa = wdpa_read(paste0(wdir, '/WDPA_Jul2023_BOL-shapefile.zip'))

wdpa=wdpa_wld_raw %>% filter(ISO3=="SLV")
# PAs are projected, and column "geometry_type" is added
wdpa_prj = wdpa_clean(wdpa, geometry_precision = 1000,
                      # Don't erase overlapped polygons
                      erase_overlaps = FALSE) %>%
  # Remove the PAs that are only proposed, or have geometry type "point"
  filter(STATUS != "Proposed") %>%
  filter(GEOMETRY_TYPE != "POINT") %>%
  # Project PA polygons to the previously determined UTM zone
  st_transform(crs = utm_code)

# Separate funded and non-funded protected areas
wdpaID_funded = paid
wdpa_funded = wdpa_prj %>% filter(WDPAID %in% wdpaID_funded) %>%
  mutate(group=1) # Assign an ID "1" to the funded PA group

wdpa_nofund = wdpa_prj %>%
  filter(!WDPAID %in% wdpaID_funded) %>%
  st_buffer(., dist=0) # a hack to solve 'bad' polygons in R, e.g. problem of self-intersection
# Determine the non-funded PAs that intersect with funded PAs,
# and delete the intersection part from non-funded PAs to reduce noise when rasterizing WDPAIDs later.
intersection = st_intersection(wdpa_nofund, wdpa_funded)
wdpa_nofund <- wdpa_nofund %>%
  { if (nrow(intersection) == 0) . else st_difference(., st_union(st_geometry(intersection))) } %>%
  mutate(group = 2)

# Make Buffers of 10km around all protected areas
wdpa_buffer <- st_buffer(wdpa_funded, dist = buffer_m) %>%
  rbind(st_buffer(wdpa_nofund, dist = buffer_m)) %>%
  # Assign an ID "3" to the buffer group
  mutate(group=3)
# Merge the dataframes of funded PAs, non-funded PAs and buffers
wdpa_groups = rbind(wdpa_funded, wdpa_nofund, wdpa_buffer)
# Subset to polygons that intersect with country boundary
wdpa.sub = wdpa_groups %>%
  st_intersects(gadm_prj, .) %>%
  unlist()
# Filter the PA+buffer to the subset
wdpa_groups = wdpa_groups[sort(wdpa.sub),] %>%
  st_as_sf()

ggplot(wdpa_groups) +
  geom_sf(aes(fill = factor(WDPAID)), lwd=0, alpha=0.4) +
  theme_bw()
```

The script proceeds by initializing a raster grid over the country's
extent, with pixel values representing group IDs. It assigns minimal
values to pixels covered by overlapping polygons, ensuring that PA group
IDs take precedence over buffer IDs. Another raster layer is created for
WDPA IDs.

Finally, the raster values are aggregated into the grid cells by taking
the mode, merged into a single dataset, and transformed to the WGS84
coordinate system to comply with the mapme.biodiversity package
requirements.

```{r}
# Initialize an empty raster to the spatial extent of the country
r.ini = raster()
extent(r.ini) = extent(gadm_prj)
# Specify the raster resolution as same as the pre-defined 'gridSize'
res(r.ini) = gridSize
# Assign the raster pixels with "Group" values,
# Take the minimal value if a pixel is covered by overlapped polygons, so that PA Group ID has higher priority than Buffer ID.
# Assign value "0" to the background pixels (control candidates group)
r.group = rasterize(wdpa_groups, r.ini, field="group", fun="min", background=0) %>%
  mask(., gadm_prj)
# Rename Layer
names(r.group) = "group"
# Rasterize wdpaid
r.wdpaid = rasterize(wdpa_prj, r.ini, field="WDPAID", fun="first", background=0) %>%
  mask(., gadm_prj)
names(r.wdpaid) = "wdpaid"

# Aggregate pixel values by taking the majority
grid.group = exact_extract(x=r.group, y=grid, fun='mode', append_cols="gridID") %>%
  rename(group = mode)
grid.wdpaid = exact_extract(x=r.wdpaid, y=grid, fun="mode", append_cols="gridID") %>%
  rename(wdpaid = mode)
# Merge data frames
grid.param = grid.group %>%
  merge(., grid.wdpaid, by="gridID") %>%
  merge(., grid, by="gridID") %>%
  # drop rows having "NA" in column "group"
  drop_na(group) %>%
  # drop the column of "gridID"
  subset(., select=-c(gridID)) %>%
  st_as_sf() %>%
  # Grid is projected to WGS84 because mapme.biodiverty package merely works with this CRS
  st_transform(crs=4326)
```

You can now visualize the gridding of the country and the attribution of
group to each cell. To do so add the right dataframe the "factor()" and
choose colors :

```{r}
# Visualize grouped grid cells
grid.param %>%
  ggplot() +
  geom_sf(aes(fill = factor(group)), lwd=0) + # add the right dataframe in factor()
  scale_fill_manual(name="Group", # legend title
                    values = c("grey", "darkgreen", "darkblue", "orange"),#add colors
                    labels = c("control candidate", "treatment candidate", "non-funded PA", "buffer zone")) +
  theme_bw()
```

## Download the data

Once a group have been attributed to each cells, all the covariates need
to be downloaded. To do so you can use the Mapme.biodiversity package
that enable you to download multiple ressources and then compute the
right indicators. Please refer to this page
(<https://github.com/mapme-initiative/mapme.biodiversity>) to find the
function to complete the following code. You will be ask to : - Download
the soil data - Calculate elevation - Download travel time data and
compute the indicators

```{r}
## 4. Calculating deforestation area and other covariates for all observation units
# Get input data ready for indicator calculation
      ## Version of Global Forest Cover data to consider
      list_version_gfc = mapme.biodiversity:::.available_gfw_versions() #all versions available
      version_gfc = list_version_gfc[length(list_version_gfc)] #last version considered
#years=2000:2021
mapme_options(outdir =wdir)
# aoi = init_portfolio(grid.param,
#                      years = 2000:2021,
#                      outdir = wdir,
#                      tmpdir = file.path(wdir, "tmp"),
#                      add_resources = FALSE)

years = 2000:2021
aoi=grid.param
```

### Covariate: Soil

Insert the function to download soil data, the layers used should be
"clay", the depth between 0 and 5cm and the statistic the mean :

```{r}
get.soil = aoi %>% get_resources(get_soilgrids(
        layers = "clay", # resource specific argument
        depths = "0-5cm", # resource specific argument
        stats = "mean"))
                         
# set up parallel plan with 6 concurrent threads
plan(multisession, workers = 20)
# Calculate Indicator
with_progress({
  zonal.soil = get.soil %>% calc_indicators(
          calc_soilproperties(
            stats = "mean",
            engine = "zonal"
          )
        )
})
plan(sequential) # close child processes
# Transform the output dataframe into a pivot dataframe
pivot.soil = zonal.soil %>%
  unnest(soilproperties) %>%
  mutate(across(value, round, 3)) %>% # Round numeric columns
  pivot_wider(names_from = c("variable"), values_from = "value")%>%
  dplyr::select(-c(datetime,unit))
      

```

### Covariate: Elevation

Insert the function to calculate elevation mean, you should use the mean
statistic and the exactextract engine :

```{r}
get.elevation = aoi %>% get_resources(get_nasa_srtm())
# set up parallel plan with 6 concurrent threads
plan(multisession, workers = 20)

# Calculate Indicator
with_progress({
  zonal.elevation = get.elevation %>% calc_indicators(calc_elevation(
          stats= "mean",
          engine = "exactextract"))
  
  
})
plan(sequential) # close child processes
# Transform the output dataframe into a pivot dataframe
pivot.elevation = zonal.elevation %>% unnest(elevation)%>%
  pivot_wider(names_from = c("variable"), values_from = "value")%>%
  dplyr::select(-c(datetime,unit))
      


```

### Covariate: TRI

```{r}
# set up parallel plan with 6 concurrent threads
plan(multisession, workers = 20)
# Calculate Indicator
with_progress({
  zonal.tri = get.elevation %>% calc_indicators(calc_tri(
          stats = "mean",
          engine = "exactextract"))
})
plan(sequential) # close child processes
# Transform the output dataframe into a pivot dataframe
pivot.tri = zonal.tri %>% unnest(tri)%>%
  pivot_wider(names_from = c("variable"), values_from = "value")%>%
  dplyr::select(-c(datetime,unit))


```

### Covariate: Travel Time

Download data related to travel time and calculate the median travel
time

```{r}
get.travelT = aoi%>% get_resources(get_nelson_et_al(ranges = c("5k_110mio")))

# set up parallel plan with 6 concurrent threads
plan(multisession, workers = 20)
# Calculate Indicator
with_progress({
  zonal.travelT  <-get.travelT %>% calc_indicators(calc_traveltime(
          stats = "median",
          engine = "exactextract"))
  })
plan(sequential) # close child processes
# Transform the output dataframe into a pivot dataframe
pivot.travelT = zonal.travelT %>%
  unnest(traveltime) %>%
  pivot_wider(names_from = "variable", values_from = "value")%>%
  dplyr::select(-c(datetime,unit))

```

### Time Series of Tree Cover Area

```{r}
get.tree = aoi %>%get_resources(get_gfw_treecover(version =  version_gfc),
                                     get_gfw_lossyear(version = version_gfc))
# set up parallel plan with 6 concurrent threads
plan(multisession, workers = 20)
# Calculate time series
with_progress({
  zonal.tree = get.tree %>% calc_indicators(calc_treecover_area(years=years, min_size=0.5, 
                                                                    min_cover=10))
  
})
plan(sequential) # close child processes

# Transform the output dataframe into a pivot dataframe
pivot.tree = zonal.tree %>%
  unnest(treecover_area) %>%
  # Transfer treecover unit to percentage
  mutate(value = round((value*1e4)/(gridSize^2)*100, 2),datetime=format(datetime, "%Y")) %>%
  pivot_wider(names_from = "datetime", values_from = "value", names_prefix = "treecover_")%>%
  dplyr::select(-c(unit,variable))

```

Q5 : Why is a multisession used ? Explain

```{r}
# The calculation of tree loss area is performed at dataframe base
# Get the column names of tree cover time series
colnames_tree = names(pivot.tree)[startsWith(names(pivot.tree), "treecover")]
# Drop the first year
dropFirst = tail(colnames_tree, -1)
# Drop the last year
dropLast = head(colnames_tree, -1)
# Set list of new column names for tree loss time series
colnames_loss = dropFirst %>% str_split(., "_")
# Add new columns: treeloss_tn = treecover_tn - treecover_t(n-1)
for (i in 1:length(dropFirst)) {
  new_colname <- paste0("treeloss_", colnames_loss[[i]][2])
  pivot.tree[[new_colname]] <- pivot.tree[[dropFirst[i]]] - pivot.tree[[dropLast[i]]]
}


# Export Matching Frame
# Remove "geometry" column from pivot dataframes
df.tree = pivot.tree %>% mutate(x = NULL) %>% as.data.frame()
df.travelT = pivot.travelT %>% mutate(x = NULL) %>% as.data.frame()
df.soil = pivot.soil %>% mutate(x = NULL) %>% as.data.frame()
df.elevation = pivot.elevation %>% mutate(x = NULL) %>% as.data.frame()
df.tri = pivot.tri %>% mutate(x=NULL) %>% as.data.frame()
# Make a dataframe containing only "assetid" and geometry
df.geom = pivot.tree[, c("assetid", "x")] %>% as.data.frame()
# Merge all output dataframes
pivot.all = Reduce(dplyr::full_join, list(df.travelT, df.soil, df.tree, df.elevation, df.tri, df.geom)) %>%
  st_as_sf()
# Make column Group ID and WDPA ID have data type "integer"
pivot.all$group = as.integer(pivot.all$group)
pivot.all$wdpaid = as.integer(pivot.all$wdpaid)

# Export the matching frame
st_write(pivot.all, dsn = file.path(wdir, name_output), delete_dsn = TRUE)

```

# Create an interactive map

```{r}
# Définir les couleurs de contour pour chaque groupe
contour_colors <- c(
  `0` = "grey",
  `1` = "darkgreen",
  `2` = "darkblue",
  `3` = "orange"
)

# Define UI
ui <- fluidPage(
  titlePanel("Visualisation des Pixels"),
  sidebarLayout(
    sidebarPanel(
      selectInput("param", 
                  "Choisir le paramètre à afficher :", 
                  choices = names(pivot.all)[!(names(pivot.all) %in% c("x", "group","wdpaid","assetid"))])
    ),
    mainPanel(
      plotOutput("mapPlot")
    )
  )
)

# Define server logic
server <- function(input, output) {
  
  output$mapPlot <- renderPlot({
    # Créer la carte avec ggplot2
    ggplot() +
      geom_sf(data = pivot.all, aes(fill = !!sym(input$param)), color = "black") +
      scale_fill_viridis_c() +  # Palette de couleurs pour le paramètre
      scale_color_manual(values = contour_colors) +
      theme_minimal() +
      labs(fill = input$param, title = "Carte des pixels avec contours colorés") +
      theme(legend.position = "right")
  })
}

# Run the application 
shinyApp(ui = ui, server = server)
```

```{r}
ggplot() +
  geom_sf(data = pivot.all, aes(fill = treecover_2000, color = as.factor(group)), size = 0.3) +
  scale_fill_viridis_c() +  # Palette de couleurs pour le paramètre
  scale_color_manual(values = contour_colors) +  # Couleurs des contours pour chaque groupe
  theme_minimal() +
  labs(fill = "Tree Cover 2000", color = "Group", title = "Carte des pixels avec contours colorés") +
  theme(legend.position = "right")

```

```{r}
worldMaps <- function(df, world_data, data_type, period, indicator){
  
  # Function for setting the aesthetics of the plot
  my_theme <- function () { 
    theme_bw() + theme(axis.text = element_text(size = 14),
                       axis.title = element_text(size = 14),
                       strip.text = element_text(size = 14),
                       panel.grid.major = element_blank(), 
                       panel.grid.minor = element_blank(),
                       panel.background = element_blank(), 
                       legend.position = "bottom",
                       panel.border = element_blank(), 
                       strip.background = element_rect(fill = 'white', colour = 'white'))
  }
  
  # Select only the data that the user has selected to view
  plotdf <- df[df$Indicator == indicator & df$DataType == data_type & df$Period == period,]
  plotdf <- plotdf[!is.na(plotdf$ISO3), ]
  
  # Add the data the user wants to see to the geographical world data
  world_data['DataType'] <- rep(data_type, nrow(world_data))
  world_data['Period'] <- rep(period, nrow(world_data))
  world_data['Indicator'] <- rep(indicator, nrow(world_data))
  world_data['Value'] <- plotdf$Value[match(world_data$ISO3, plotdf$ISO3)]
  
  # Create caption with the data source to show underneath the map
  capt <- paste0("Source: ", ifelse(data_type == "Childlessness", "United Nations" , "World Bank"))
  
  # Specify the plot for the world map
  library(RColorBrewer)
  library(ggiraph)
  g <- ggplot() + 
    geom_polygon_interactive(data = world_data, color = 'gray70', size = 0.1,
                                    aes(x = long, y = lat, fill = Value, group = group, 
                                        tooltip = sprintf("%s<br/>%s", ISO3, Value))) + 
    scale_fill_gradientn(colours = brewer.pal(5, "RdBu"), na.value = 'white') + 
    scale_y_continuous(limits = c(-60, 90), breaks = c()) + 
    scale_x_continuous(breaks = c()) + 
    labs(fill = data_type, color = data_type, title = NULL, x = NULL, y = NULL, caption = capt) + 
    my_theme()
  
  return(g)
}
```
